{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291a8666-45f5-4845-9ab1-c9701ab7f04f",
   "metadata": {
    "id": "291a8666-45f5-4845-9ab1-c9701ab7f04f"
   },
   "source": [
    "### this project to use GEN LLM Model and apply RAG technique\n",
    "to help junior ML engineer to learn the Designing of Machine Learning System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47c461-2e1e-4455-b48f-8d4f2ae58d7b",
   "metadata": {
    "id": "1d47c461-2e1e-4455-b48f-8d4f2ae58d7b"
   },
   "source": [
    "### 1- first step to read the data from the pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b05cb-7f36-4d08-82e8-57b52b210335",
   "metadata": {
    "id": "ea9b05cb-7f36-4d08-82e8-57b52b210335"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e7c13-d83d-47d2-90ed-5a07ffa6d445",
   "metadata": {
    "id": "a31e7c13-d83d-47d2-90ed-5a07ffa6d445"
   },
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def clean_the_text(text: str) -> str:\n",
    "    \"\"\"Clean PDF page text by removing headers, footers, page numbers, and fixing hyphenations.\"\"\"\n",
    "    # Remove newlines and normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove common page numbers (optional, tweak as needed)\n",
    "    text = re.sub(r'Page \\d+|\\d{1,3}$', '', text)\n",
    "\n",
    "    # Fix hyphenation at line breaks: \"exam-\\nple\" => \"example\"\n",
    "    text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "    # Optional: remove figure/table labels\n",
    "    text = re.sub(r'Figure\\s*\\d+[^.]*\\.?', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'Table\\s*\\d+[^.]*\\.?', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def get_pdf_files_name(folder_path:str) -> list[str] :\n",
    "    \"\"\"Get all pdf file paths in the given folder\"\"\"\n",
    "    return [os.path.join(folder_path,f) for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "\n",
    "def getDataFromPdf(pdfbook:str) ->list[dict] :\n",
    "    \"\"\"This function used to get the book pages contains\"\"\"\n",
    "    #  open the pdf file to get the pages and page number\n",
    "    doc = fitz.open(pdfbook)\n",
    "    pdfData= []\n",
    "    for page_number, page in tqdm(enumerate(doc)):\n",
    "       # get the data from the page\n",
    "        text_in_apage = page.get_text()\n",
    "        #clean the text\n",
    "        cleanedText = clean_the_text(text_in_apage)\n",
    "        pdfData.append({\n",
    "            \"book name\": pdfbook.replace(\"./bookpdf\\\\\",\"\"),\n",
    "            \"page_number\" : page_number,\n",
    "             \"numberOfCharPerPage\" :len(cleanedText),\n",
    "            \"numberOfWordPerPage\":len(cleanedText.split(\" \")),\n",
    "            \"numberOfTokenPerPageExpected\":len(re.sub(r\"[\\s]\", \"\", cleanedText))/4,\n",
    "            \"numberOfSentences\":len(cleanedText.split(\". \")),\n",
    "            \"text\":cleanedText\n",
    "                   })\n",
    "    return pdfData\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1629723b-3c1a-4cac-9e5d-d3f6ba820995",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "a1aa4228bc8641bab239a0ae974aeb4e",
      "bf76c5b720e849b7bdcecda0018c5080",
      "6bbe9fb49f26466bbfcaf580ecf32b79",
      "0e3d839637974536a90c9574a5c8cdba",
      "696460c7fa9b411e82e459a5dce91138"
     ]
    },
    "id": "1629723b-3c1a-4cac-9e5d-d3f6ba820995",
    "outputId": "a6acd018-8d94-4acd-81d1-c566bb5f1669",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./bookpdf\\\\Computer-Vision-Algorithms-and-Applications-2nd Edition, Richard Szeliski.pdf', './bookpdf\\\\Data-Science-and-Machine-Learning.pdf', './bookpdf\\\\designing-machine-learning-systems.pdf', './bookpdf\\\\Information-Theory,-Inference,-and-Learning-Algorithms.pdf', './bookpdf\\\\Introduction-to-Machine-Learning-with-Python.pdf']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1aa4228bc8641bab239a0ae974aeb4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf76c5b720e849b7bdcecda0018c5080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbe9fb49f26466bbfcaf580ecf32b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3d839637974536a90c9574a5c8cdba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696460c7fa9b411e82e459a5dce91138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n"
     ]
    }
   ],
   "source": [
    "# === Main execution ===\n",
    "folder_path = \"./bookpdf\"  # replace with your folder path\n",
    "pdf_files = get_pdf_files_name(folder_path)\n",
    "print(pdf_files)\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    book_data = getDataFromPdf(pdf_file)\n",
    "    print(len(book_data))\n",
    "    all_data.extend(book_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b636726-c5ac-44e2-91ae-2b3a5556b90d",
   "metadata": {
    "id": "1b636726-c5ac-44e2-91ae-2b3a5556b90d",
    "outputId": "732b2b21-6e11-4d7f-eda2-fb116c340300"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3267"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd9ac50-c24f-476c-8839-a34e8d890178",
   "metadata": {
    "id": "2cd9ac50-c24f-476c-8839-a34e8d890178",
    "outputId": "29a3798b-cc66-48d7-945a-d759344e0347"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book name': 'Computer-Vision-Algorithms-and-Applications-2nd Edition, Richard Szeliski.pdf',\n",
       " 'page_number': 500,\n",
       " 'numberOfCharPerPage': 2010,\n",
       " 'numberOfWordPerPage': 334,\n",
       " 'numberOfTokenPerPageExpected': 419.25,\n",
       " 'numberOfSentences': 12,\n",
       " 'text': '7.3 Contour tracking 475 -1 +1 ϕ = 0 ϕ∆ g(I) 44 Level set evolution for a geodesic active contour. The embedding function φ is updated based on the curvature of the underlying surface modulated by the edge/speed function g(I), as well as the gradient of g(I), thereby attracting it to strong edges. Kimmel, and Sapiro (1997) and Yezzi, Kichenassamy et al. (1997), dφ dt = |∇φ|div \\x12 g(I) ∇φ |∇φ| \\x13 = g(I)|∇φ|div \\x12 ∇φ |∇φ| \\x13 + ∇g(I) · ∇φ, (7.38) where g(I) is a generalized version of the snake edge potential. To get an intuitive sense of the curve’s behavior, assume that the embedding function φ is a signed distance function away from the curve (44), in which case |φ| = 1. The ﬁrst term in Equation (7.38) moves the curve in the direction of its curvature, i.e., it acts to straighten the curve, under the inﬂuence of the modulation function g(I). The second term moves the curve down the gradient of g(I), encouraging the curve to migrate towards minima of g(I). While this level-set formulation can readily change topology, it is still susceptible to local minima, since it is based on local measurements such as image gradients. An alternative approach is to re-cast the problem in a segmentation framework, where the energy measures the consistency of the image statistics (e.g., color, texture, motion) inside and outside the segmented regions (Cremers, Rousson, and Deriche 2007; Rousson and Paragios 2008; Houhou, Thiran, and Bresson 2008). These approaches build on earlier energy-based segmentation frameworks introduced by Leclerc (1989), Mumford and Shah (1989), and Chan and Vese (2001), which are discussed in more detail in Section 4.3.2. For more information on level sets and their applications, please see the collection of papers edited by Osher and Paragios (2003) as well as the series of Workshops on Variational and Level Set Methods in Computer Vision (Paragios, Faugeras et al. 2005) and Special Issues on Scale Space and Variational Methods in Computer Vision (Paragios and Sgallari'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8616848-3e4f-4211-a5ff-0fbe716db81b",
   "metadata": {
    "id": "c8616848-3e4f-4211-a5ff-0fbe716db81b"
   },
   "source": [
    "### 2- Data preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e45e78-5c14-4b50-8049-109e5f95251e",
   "metadata": {
    "id": "f4e45e78-5c14-4b50-8049-109e5f95251e"
   },
   "source": [
    "we need to get some information about the data like the maximum number of words, token, ... which will help us to select the proper LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843b9de-e0da-4aeb-af1a-80d23a5b4777",
   "metadata": {
    "id": "d843b9de-e0da-4aeb-af1a-80d23a5b4777"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4 pandas==2.2.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e32121-365b-420f-acee-6dffc7a67e9c",
   "metadata": {
    "id": "78e32121-365b-420f-acee-6dffc7a67e9c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d4469-4820-42f5-90b7-082207e273f2",
   "metadata": {
    "id": "566d4469-4820-42f5-90b7-082207e273f2",
    "outputId": "a67354ce-2aa3-4463-d2ee-53bb49e7d134"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>numberOfCharPerPage</th>\n",
       "      <th>numberOfWordPerPage</th>\n",
       "      <th>numberOfTokenPerPageExpected</th>\n",
       "      <th>numberOfSentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3267.00</td>\n",
       "      <td>3267.00</td>\n",
       "      <td>3267.00</td>\n",
       "      <td>3267.00</td>\n",
       "      <td>3267.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>395.29</td>\n",
       "      <td>2144.34</td>\n",
       "      <td>356.56</td>\n",
       "      <td>447.20</td>\n",
       "      <td>29.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>305.14</td>\n",
       "      <td>810.24</td>\n",
       "      <td>150.24</td>\n",
       "      <td>169.73</td>\n",
       "      <td>75.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>163.00</td>\n",
       "      <td>1692.50</td>\n",
       "      <td>276.00</td>\n",
       "      <td>350.00</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>326.00</td>\n",
       "      <td>2175.00</td>\n",
       "      <td>361.00</td>\n",
       "      <td>451.00</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>529.00</td>\n",
       "      <td>2704.00</td>\n",
       "      <td>433.00</td>\n",
       "      <td>562.75</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1231.00</td>\n",
       "      <td>11232.00</td>\n",
       "      <td>2436.00</td>\n",
       "      <td>2199.25</td>\n",
       "      <td>1082.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  numberOfCharPerPage  numberOfWordPerPage  \\\n",
       "count      3267.00              3267.00              3267.00   \n",
       "mean        395.29              2144.34               356.56   \n",
       "std         305.14               810.24               150.24   \n",
       "min           0.00                 0.00                 1.00   \n",
       "25%         163.00              1692.50               276.00   \n",
       "50%         326.00              2175.00               361.00   \n",
       "75%         529.00              2704.00               433.00   \n",
       "max        1231.00             11232.00              2436.00   \n",
       "\n",
       "       numberOfTokenPerPageExpected  numberOfSentences  \n",
       "count                       3267.00            3267.00  \n",
       "mean                         447.20              29.39  \n",
       "std                          169.73              75.17  \n",
       "min                            0.00               1.00  \n",
       "25%                          350.00              11.00  \n",
       "50%                          451.00              17.00  \n",
       "75%                          562.75              25.00  \n",
       "max                         2199.25            1082.00  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ca848-461e-46b0-897c-bae8fc89db5e",
   "metadata": {
    "id": "060ca848-461e-46b0-897c-bae8fc89db5e"
   },
   "source": [
    " Note: the mean of number of token ~= 450\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee74ec0-8ec1-4724-90d5-624a001fbafd",
   "metadata": {
    "id": "2ee74ec0-8ec1-4724-90d5-624a001fbafd"
   },
   "source": [
    "### 2.1 divide the page into sentences for easier to handle\n",
    "use SpaCy or NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08dbac-ea75-4d3a-b5ec-f3e40b294082",
   "metadata": {
    "id": "5d08dbac-ea75-4d3a-b5ec-f3e40b294082"
   },
   "outputs": [],
   "source": [
    "# !pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58691d17-0738-4111-9d7a-5df45843b580",
   "metadata": {
    "id": "58691d17-0738-4111-9d7a-5df45843b580"
   },
   "outputs": [],
   "source": [
    "# Use nltk.tokenize.PunktSentenceTokenizer\n",
    "# This is often better than spaCy's sentencizer if you want lightweight segmentation without full parsing\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# item[\"sentences\"] = sent_tokenize(item[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0543b0-a3f6-4388-b4e5-fe79c13756a9",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "ed2d4611198d477fa5d764cdaaf3517a"
     ]
    },
    "id": "ef0543b0-a3f6-4388-b4e5-fe79c13756a9",
    "outputId": "04187c74-2845-4076-95ef-f34854280c57"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2d4611198d477fa5d764cdaaf3517a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# use Spacy for divide pages to sentences then add it to the bookData\n",
    "\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "for item in tqdm(all_data):\n",
    "    #convert the page text into sentences and make sure that the sentences datatupe is string\n",
    "    item[\"sentences\"] = [str(sent) for sent in nlp(item[\"text\"]).sents]\n",
    "\n",
    "    # item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "    # item[\"sentences\"] =[str(sentence) for sentence in item[\"sentences\"]]\n",
    "    # Count the sentences\n",
    "    item[\"sentences_per_page_spacy\"] = len(item[\"sentences\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb810533-ab2d-4486-958a-4664fc5e0fde",
   "metadata": {
    "id": "bb810533-ab2d-4486-958a-4664fc5e0fde",
    "outputId": "33e3685b-4aaa-4652-c9c7-8dd8a6985336"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book name': 'Computer-Vision-Algorithms-and-Applications-2nd Edition, Richard Szeliski.pdf',\n",
       " 'page_number': 45,\n",
       " 'numberOfCharPerPage': 1726,\n",
       " 'numberOfWordPerPage': 264,\n",
       " 'numberOfTokenPerPageExpected': 365.75,\n",
       " 'numberOfSentences': 15,\n",
       " 'text': '20 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021) (a) (b) (c) (d) (e) (f) 11 Examples of computer vision algorithms from the 2010s: (a) the SuperVision deep neural network © Krizhevsky, Sutskever, and Hinton (2012); (b) object instance segmentation (He, Gkioxari et al. 2017) © 2017 IEEE; (c) whole body, expression, and gesture ﬁtting from a single image (Pavlakos, Choutas et al. 2019) © 2019 IEEE; (d) fusing multiple color depth images using the KinectFusion real-time system (Newcombe, Izadi et al. 2011) © 2011 IEEE; (e) smartphone augmented reality with real-time depth occlusion effects (Valentin, Kowdle et al. 2018) © 2018 ACM; (f) 3D map computed in real-time on a fully autonomous Skydio R1 drone (Cross 2019). machine learning techniques to computer vision problems (see Chapters 5 and 6). This trend coincided with the increased availability of immense quantities of partially labeled data on the internet, as well as signiﬁcant increases in computational power, which makes it more feasible to learn object categories without the use of careful human supervision. 2010s. The trend towards using large labeled (and also self-supervised) datasets to develop machine learning algorithms became a tidal wave that totally revolutionized the development of image recognition algorithms as well as other applications, such as denoising and optical ﬂow, which previously used Bayesian and global optimization techniques. This trend was enabled by the development of high-quality large-scale annotated datasets such as ImageNet (Deng, Dong et al. 2009; Russakovsky, Deng et al. 2015), Microsoft COCO (Common Objects in Context) (Lin, Maire et al. 2014), and LVIS (Gupta, Doll´ar, and Gir-',\n",
       " 'sentences': ['20 Computer Vision: Algorithms and Applications, 2nd ed. (',\n",
       "  'ﬁnal draft, Sept. 2021) (a) (b) (c) (d) (e) (f) 11 Examples of computer vision algorithms from the 2010s: (a) the SuperVision deep neural network © Krizhevsky, Sutskever, and Hinton (2012); (b) object instance segmentation (He, Gkioxari et al.',\n",
       "  '2017) © 2017 IEEE; (c) whole body, expression, and gesture ﬁtting from a single image (Pavlakos, Choutas et al.',\n",
       "  '2019) © 2019 IEEE; (d) fusing multiple color depth images using the KinectFusion real-time system (Newcombe, Izadi et al.',\n",
       "  '2011) © 2011 IEEE; (e) smartphone augmented reality with real-time depth occlusion effects (Valentin, Kowdle et al.',\n",
       "  '2018) © 2018 ACM; (f) 3D map computed in real-time on a fully autonomous Skydio R1 drone (Cross 2019).',\n",
       "  'machine learning techniques to computer vision problems (see Chapters 5 and 6).',\n",
       "  'This trend coincided with the increased availability of immense quantities of partially labeled data on the internet, as well as signiﬁcant increases in computational power, which makes it more feasible to learn object categories without the use of careful human supervision.',\n",
       "  '2010s.',\n",
       "  'The trend towards using large labeled (and also self-supervised) datasets to develop machine learning algorithms became a tidal wave that totally revolutionized the development of image recognition algorithms as well as other applications, such as denoising and optical ﬂow, which previously used Bayesian and global optimization techniques.',\n",
       "  'This trend was enabled by the development of high-quality large-scale annotated datasets such as ImageNet (Deng, Dong et al.',\n",
       "  '2009; Russakovsky, Deng et al.',\n",
       "  '2015), Microsoft COCO (Common Objects in Context) (Lin, Maire et al.',\n",
       "  '2014), and LVIS (Gupta, Doll´ar, and Gir-'],\n",
       " 'sentences_per_page_spacy': 14}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a141c181-15e5-4ad3-a92c-94ca5c78e774",
   "metadata": {
    "id": "a141c181-15e5-4ad3-a92c-94ca5c78e774"
   },
   "source": [
    "#### 2.2 Check and study the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e6627-810b-465e-b774-fe2d488f5692",
   "metadata": {
    "id": "e13e6627-810b-465e-b774-fe2d488f5692"
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8d492b-ba46-4403-aa5f-6cb97d270318",
   "metadata": {
    "id": "db8d492b-ba46-4403-aa5f-6cb97d270318",
    "outputId": "ab6c5f2f-899b-4957-d262-50d2b38fbc62"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>numberOfCharPerPage</th>\n",
       "      <th>numberOfWordPerPage</th>\n",
       "      <th>numberOfTokenPerPageExpected</th>\n",
       "      <th>numberOfSentences</th>\n",
       "      <th>sentences_per_page_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3267.00</td>\n",
       "      <td>3267.00</td>\n",
       "      <td>3267.00</td>\n",
       "      <td>3267.00</td>\n",
       "      <td>3267.00</td>\n",
       "      <td>3267.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>395.29</td>\n",
       "      <td>2144.34</td>\n",
       "      <td>356.56</td>\n",
       "      <td>447.20</td>\n",
       "      <td>29.39</td>\n",
       "      <td>19.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>305.14</td>\n",
       "      <td>810.24</td>\n",
       "      <td>150.24</td>\n",
       "      <td>169.73</td>\n",
       "      <td>75.17</td>\n",
       "      <td>14.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>163.00</td>\n",
       "      <td>1692.50</td>\n",
       "      <td>276.00</td>\n",
       "      <td>350.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>326.00</td>\n",
       "      <td>2175.00</td>\n",
       "      <td>361.00</td>\n",
       "      <td>451.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>529.00</td>\n",
       "      <td>2704.00</td>\n",
       "      <td>433.00</td>\n",
       "      <td>562.75</td>\n",
       "      <td>25.00</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1231.00</td>\n",
       "      <td>11232.00</td>\n",
       "      <td>2436.00</td>\n",
       "      <td>2199.25</td>\n",
       "      <td>1082.00</td>\n",
       "      <td>175.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  numberOfCharPerPage  numberOfWordPerPage  \\\n",
       "count      3267.00              3267.00              3267.00   \n",
       "mean        395.29              2144.34               356.56   \n",
       "std         305.14               810.24               150.24   \n",
       "min           0.00                 0.00                 1.00   \n",
       "25%         163.00              1692.50               276.00   \n",
       "50%         326.00              2175.00               361.00   \n",
       "75%         529.00              2704.00               433.00   \n",
       "max        1231.00             11232.00              2436.00   \n",
       "\n",
       "       numberOfTokenPerPageExpected  numberOfSentences  \\\n",
       "count                       3267.00            3267.00   \n",
       "mean                         447.20              29.39   \n",
       "std                          169.73              75.17   \n",
       "min                            0.00               1.00   \n",
       "25%                          350.00              11.00   \n",
       "50%                          451.00              17.00   \n",
       "75%                          562.75              25.00   \n",
       "max                         2199.25            1082.00   \n",
       "\n",
       "       sentences_per_page_spacy  \n",
       "count                   3267.00  \n",
       "mean                      19.08  \n",
       "std                       14.11  \n",
       "min                        0.00  \n",
       "25%                       11.00  \n",
       "50%                       17.00  \n",
       "75%                       24.00  \n",
       "max                      175.00  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced865df-0723-4ed5-8768-671ac916f221",
   "metadata": {
    "id": "ced865df-0723-4ed5-8768-671ac916f221"
   },
   "source": [
    "#### 2.3 Divide the list of sentences per page into smaller, discrete units for easier processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82adc0ba-9c2c-495e-8997-6dcdc2c18565",
   "metadata": {
    "id": "82adc0ba-9c2c-495e-8997-6dcdc2c18565"
   },
   "source": [
    "- Enhanced Manageability: Dividing text into smaller, evenly sized chunks ensures easier handling and processing, especially when dealing with large datasets.\n",
    "\n",
    "- Avoiding Information Loss: Embedding models often have a token capacity limit (e.g., 384 tokens). If a sequence exceeds this limit, information loss may occur due to truncation, which compromises the quality of embeddings.\n",
    "\n",
    "- Optimal Utilization of LLM Context Window: Large Language Models (LLMs) typically have a restricted context window, which dictates the number of tokens they can process at once. Exceeding this capacity not only leads to inefficiencies but also requires additional computational resources. By chunking text appropriately, we maximize the utility of the context window while minimizing unnecessary computational overhead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bebb94d-4fa4-46d0-9f92-fae0effad384",
   "metadata": {
    "id": "6bebb94d-4fa4-46d0-9f92-fae0effad384"
   },
   "outputs": [],
   "source": [
    "chunk_size = 10  # number of sentences in one chunck (e.g [18 sentences]->[[10sents],[8sents]])\n",
    "\n",
    "def split_list(input_list : list, chunk_size:int) ->list[list] :\n",
    "\n",
    "   \"\"\"\n",
    "    Splits a list into smaller sublists of a specified maximum size.\n",
    "\n",
    "    Parameters:\n",
    "        input_list (List[str]): The list to be split.\n",
    "        chunk_size (int): The maximum size of each sublist.\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: A list of sublists, each containing up to `chunk_size` elements.\n",
    "   \"\"\"\n",
    "   return [input_list[i : i + chunk_size] for i in range(0 , len(input_list), chunk_size)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c34286-d43c-4880-a329-bf9a4d8f5159",
   "metadata": {
    "id": "12c34286-d43c-4880-a329-bf9a4d8f5159",
    "outputId": "6e289971-57f4-4a0d-f070-432198894362"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['30 Computer Vision: Algorithms and Applications, 2nd ed. (',\n",
       " 'ﬁnal draft, Sept. 2021) Week Chapter Topics 1.',\n",
       " 'Chapters 1–2 Introduction and image formation 2.',\n",
       " 'Chapter 3 Image processing 3.',\n",
       " 'Chapters 4–5 Optimization and learning 4.',\n",
       " 'Chapter 5 Deep learning 5.',\n",
       " 'Chapter 6 Recognition 6.',\n",
       " 'Chapter 7 Feature detection and matching 7.',\n",
       " 'Chapter 8 Image alignment and stitching 8.',\n",
       " 'Chapter 9 Motion estimation 9.',\n",
       " 'Chapter 10 Computational photography 10.',\n",
       " 'Chapter 11 Structure from motion 11.',\n",
       " 'Chapter 12 Depth estimation 12.',\n",
       " 'Chapter 13 3D reconstruction 13.',\n",
       " 'Chapter 14 Image-based rendering 1 Sample syllabus for a one semester 13-week course.',\n",
       " 'A 10-week quarter could go into lesser depth or omit some topics.',\n",
       " '1.4 Sample syllabus Teaching all of the material covered in this book in a single quarter or semester course is a Herculean task and likely one not worth attempting.11 It is better to simply pick and choose topics related to the lecturer’s preferred emphasis and tailored to the set of mini-projects envisioned for the students.',\n",
       " 'Steve Seitz and I have successfully used a 10-week syllabus similar to the one shown in 1 as both an undergraduate and a graduate-level course in computer vision.',\n",
       " 'The undergraduate course12 tends to go lighter on the mathematics and takes more time reviewing basics, while the graduate-level course13 dives more deeply into techniques and assumes the students already have a decent grounding in either vision or related mathematical techniques.',\n",
       " 'Related courses have also been taught on the topics of 3D photography and computational photography.',\n",
       " 'Appendix C.3 and the book’s website list other courses that use this book to teach a similar curriculum.',\n",
       " '11Some universities, such as Stanford (CS231A & 231N), Berkeley (CS194-26/294-26 & 280), and the University of Michigan (EECS 498/598 & 442), now split the material over two courses.',\n",
       " '12http://www.cs.washington.edu/education/courses/455 13http://www.cs.washington.edu/education/courses/576']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[55]['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b977414f-b914-472d-8054-1d9a1cbccd8a",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "16bcd7cb72cb44c2836e42b509f26609"
     ]
    },
    "id": "b977414f-b914-472d-8054-1d9a1cbccd8a",
    "outputId": "7e77b896-6c87-4198-ff08-dd1934e06416"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bcd7cb72cb44c2836e42b509f26609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop through pages and texts and split sentences into chunks\n",
    "for item in tqdm(all_data):\n",
    "    item[\"sentence_chunks\"] = split_list(item[\"sentences\"],\n",
    "                                         chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23032d37-e04a-4ead-8f78-eee97ad3bf59",
   "metadata": {
    "id": "23032d37-e04a-4ead-8f78-eee97ad3bf59",
    "outputId": "272d69b1-3f9d-45fd-fadf-39a884245345"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book name': 'Computer-Vision-Algorithms-and-Applications-2nd Edition, Richard Szeliski.pdf',\n",
       " 'page_number': 52,\n",
       " 'numberOfCharPerPage': 3125,\n",
       " 'numberOfWordPerPage': 459,\n",
       " 'numberOfTokenPerPageExpected': 666.75,\n",
       " 'numberOfSentences': 18,\n",
       " 'text': '1.3 Book overview 27 and recognition techniques are built on extracting and matching feature points (Section 7.1), so this is a fundamental technique required by many subsequent chapters (Chapters 8 and 11) and even in instance recognition (Section 6.1). We also cover edge and straight line detection in Sections 7.2 and 7.4, contour tracking in Section 7.3, and low-level segmentation techniques in Section 7.5. Feature detection and matching are used in Chapter 8 to perform image alignment (or registration) and image stitching. We introduce the basic techniques of feature-based alignment and show how this problem can be solved using either linear or non-linear least squares, depending on the motion involved. We also introduce additional concepts, such as uncertainty weighting and robust regression, which are essential to making real-world systems work. Feature-based alignment is then used as a building block for both 2D applications such as image stitching (Section 8.2) and computational photography (Chapter 10), as well as 3D geometric alignment tasks such as pose estimation and structure from motion (Chapter 11). The second part of Chapter 8 is devoted to image stitching, i.e., the construction of large panoramas and composites. While stitching is just one example of computational photography (see Chapter 10), there is enough depth here to warrant a separate section. We start by discussing various possible motion models (Section 8.2.1), including planar motion and pure camera rotation. We then discuss global alignment (Section 8.3), which is a special (simpliﬁed) case of general bundle adjustment, and then present panorama recognition, i.e., techniques for automatically discovering which images actually form overlapping panoramas. Finally, we cover the topics of image compositing and blending (Section 8.4), which involve both selecting which pixels from which images to use and blending them together so as to disguise exposure differences. Image stitching is a wonderful application that ties together most of the material covered in earlier parts of this book. It also makes for a good mid-term course project that can build on previously developed techniques such as image warping and feature detection and matching. Sections 8.2–8.4 also present more specialized variants of stitching such as whiteboard and document scanning, video summarization, panography, full 360° spherical panoramas, and interactive photomontage for blending repeated action shots together. In Chapter 9, we generalize the concept of feature-based image alignment to cover dense intensity-based motion estimation, i.e., optical ﬂow. We start with the simplest possible motion models, translational motion (Section 9.1), and cover topics such as hierarchical (coarse-to-ﬁne) motion estimation, Fourier-based techniques, and iterative reﬁnement. We then present parametric motion models, which can be used to compensate for camera rotation and zooming, as well as afﬁne or planar perspective motion (Section 9.2). This is then generalized to spline-based motion models (Section 9.2.2) and ﬁnally to general per-pixel',\n",
       " 'sentences': ['1.3 Book overview 27 and recognition techniques are built on extracting and matching feature points (Section 7.1), so this is a fundamental technique required by many subsequent chapters (Chapters 8 and 11) and even in instance recognition (Section 6.1).',\n",
       "  'We also cover edge and straight line detection in Sections 7.2 and 7.4, contour tracking in Section 7.3, and low-level segmentation techniques in Section 7.5.',\n",
       "  'Feature detection and matching are used in Chapter 8 to perform image alignment (or registration) and image stitching.',\n",
       "  'We introduce the basic techniques of feature-based alignment and show how this problem can be solved using either linear or non-linear least squares, depending on the motion involved.',\n",
       "  'We also introduce additional concepts, such as uncertainty weighting and robust regression, which are essential to making real-world systems work.',\n",
       "  'Feature-based alignment is then used as a building block for both 2D applications such as image stitching (Section 8.2) and computational photography (Chapter 10), as well as 3D geometric alignment tasks such as pose estimation and structure from motion (Chapter 11).',\n",
       "  'The second part of Chapter 8 is devoted to image stitching, i.e., the construction of large panoramas and composites.',\n",
       "  'While stitching is just one example of computational photography (see Chapter 10), there is enough depth here to warrant a separate section.',\n",
       "  'We start by discussing various possible motion models (Section 8.2.1), including planar motion and pure camera rotation.',\n",
       "  'We then discuss global alignment (Section 8.3), which is a special (simpliﬁed) case of general bundle adjustment, and then present panorama recognition, i.e., techniques for automatically discovering which images actually form overlapping panoramas.',\n",
       "  'Finally, we cover the topics of image compositing and blending (Section 8.4), which involve both selecting which pixels from which images to use and blending them together so as to disguise exposure differences.',\n",
       "  'Image stitching is a wonderful application that ties together most of the material covered in earlier parts of this book.',\n",
       "  'It also makes for a good mid-term course project that can build on previously developed techniques such as image warping and feature detection and matching.',\n",
       "  'Sections 8.2–8.4 also present more specialized variants of stitching such as whiteboard and document scanning, video summarization, panography, full 360° spherical panoramas, and interactive photomontage for blending repeated action shots together.',\n",
       "  'In Chapter 9, we generalize the concept of feature-based image alignment to cover dense intensity-based motion estimation, i.e., optical ﬂow.',\n",
       "  'We start with the simplest possible motion models, translational motion (Section 9.1), and cover topics such as hierarchical (coarse-to-ﬁne) motion estimation, Fourier-based techniques, and iterative reﬁnement.',\n",
       "  'We then present parametric motion models, which can be used to compensate for camera rotation and zooming, as well as afﬁne or planar perspective motion (Section 9.2).',\n",
       "  'This is then generalized to spline-based motion models (Section 9.2.2) and ﬁnally to general per-pixel'],\n",
       " 'sentences_per_page_spacy': 18,\n",
       " 'sentence_chunks': [['1.3 Book overview 27 and recognition techniques are built on extracting and matching feature points (Section 7.1), so this is a fundamental technique required by many subsequent chapters (Chapters 8 and 11) and even in instance recognition (Section 6.1).',\n",
       "   'We also cover edge and straight line detection in Sections 7.2 and 7.4, contour tracking in Section 7.3, and low-level segmentation techniques in Section 7.5.',\n",
       "   'Feature detection and matching are used in Chapter 8 to perform image alignment (or registration) and image stitching.',\n",
       "   'We introduce the basic techniques of feature-based alignment and show how this problem can be solved using either linear or non-linear least squares, depending on the motion involved.',\n",
       "   'We also introduce additional concepts, such as uncertainty weighting and robust regression, which are essential to making real-world systems work.',\n",
       "   'Feature-based alignment is then used as a building block for both 2D applications such as image stitching (Section 8.2) and computational photography (Chapter 10), as well as 3D geometric alignment tasks such as pose estimation and structure from motion (Chapter 11).',\n",
       "   'The second part of Chapter 8 is devoted to image stitching, i.e., the construction of large panoramas and composites.',\n",
       "   'While stitching is just one example of computational photography (see Chapter 10), there is enough depth here to warrant a separate section.',\n",
       "   'We start by discussing various possible motion models (Section 8.2.1), including planar motion and pure camera rotation.',\n",
       "   'We then discuss global alignment (Section 8.3), which is a special (simpliﬁed) case of general bundle adjustment, and then present panorama recognition, i.e., techniques for automatically discovering which images actually form overlapping panoramas.'],\n",
       "  ['Finally, we cover the topics of image compositing and blending (Section 8.4), which involve both selecting which pixels from which images to use and blending them together so as to disguise exposure differences.',\n",
       "   'Image stitching is a wonderful application that ties together most of the material covered in earlier parts of this book.',\n",
       "   'It also makes for a good mid-term course project that can build on previously developed techniques such as image warping and feature detection and matching.',\n",
       "   'Sections 8.2–8.4 also present more specialized variants of stitching such as whiteboard and document scanning, video summarization, panography, full 360° spherical panoramas, and interactive photomontage for blending repeated action shots together.',\n",
       "   'In Chapter 9, we generalize the concept of feature-based image alignment to cover dense intensity-based motion estimation, i.e., optical ﬂow.',\n",
       "   'We start with the simplest possible motion models, translational motion (Section 9.1), and cover topics such as hierarchical (coarse-to-ﬁne) motion estimation, Fourier-based techniques, and iterative reﬁnement.',\n",
       "   'We then present parametric motion models, which can be used to compensate for camera rotation and zooming, as well as afﬁne or planar perspective motion (Section 9.2).',\n",
       "   'This is then generalized to spline-based motion models (Section 9.2.2) and ﬁnally to general per-pixel']],\n",
       " 'num_chunks': 2}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5df3a-7c5b-4450-9f51-256fff28495c",
   "metadata": {
    "id": "47e5df3a-7c5b-4450-9f51-256fff28495c"
   },
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e51492-b98e-48d4-9adf-8a4252c5d7da",
   "metadata": {
    "id": "22e51492-b98e-48d4-9adf-8a4252c5d7da",
    "outputId": "de99dcd1-2895-4b12-9add-1fa66a82f080"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>numberOfCharPerPage</th>\n",
       "      <th>numberOfWordPerPage</th>\n",
       "      <th>numberOfTokenPerPageExpected</th>\n",
       "      <th>numberOfSentences</th>\n",
       "      <th>sentences_per_page_spacy</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3267.00</td>\n",
       "      <td>3267.00</td>\n",
       "      <td>3267.00</td>\n",
       "      <td>3267.00</td>\n",
       "      <td>3267.00</td>\n",
       "      <td>3267.00</td>\n",
       "      <td>3267.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>395.29</td>\n",
       "      <td>2144.34</td>\n",
       "      <td>356.56</td>\n",
       "      <td>447.20</td>\n",
       "      <td>29.39</td>\n",
       "      <td>19.08</td>\n",
       "      <td>2.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>305.14</td>\n",
       "      <td>810.24</td>\n",
       "      <td>150.24</td>\n",
       "      <td>169.73</td>\n",
       "      <td>75.17</td>\n",
       "      <td>14.11</td>\n",
       "      <td>1.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>163.00</td>\n",
       "      <td>1692.50</td>\n",
       "      <td>276.00</td>\n",
       "      <td>350.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>326.00</td>\n",
       "      <td>2175.00</td>\n",
       "      <td>361.00</td>\n",
       "      <td>451.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>529.00</td>\n",
       "      <td>2704.00</td>\n",
       "      <td>433.00</td>\n",
       "      <td>562.75</td>\n",
       "      <td>25.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1231.00</td>\n",
       "      <td>11232.00</td>\n",
       "      <td>2436.00</td>\n",
       "      <td>2199.25</td>\n",
       "      <td>1082.00</td>\n",
       "      <td>175.00</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  numberOfCharPerPage  numberOfWordPerPage  \\\n",
       "count      3267.00              3267.00              3267.00   \n",
       "mean        395.29              2144.34               356.56   \n",
       "std         305.14               810.24               150.24   \n",
       "min           0.00                 0.00                 1.00   \n",
       "25%         163.00              1692.50               276.00   \n",
       "50%         326.00              2175.00               361.00   \n",
       "75%         529.00              2704.00               433.00   \n",
       "max        1231.00             11232.00              2436.00   \n",
       "\n",
       "       numberOfTokenPerPageExpected  numberOfSentences  \\\n",
       "count                       3267.00            3267.00   \n",
       "mean                         447.20              29.39   \n",
       "std                          169.73              75.17   \n",
       "min                            0.00               1.00   \n",
       "25%                          350.00              11.00   \n",
       "50%                          451.00              17.00   \n",
       "75%                          562.75              25.00   \n",
       "max                         2199.25            1082.00   \n",
       "\n",
       "       sentences_per_page_spacy  num_chunks  \n",
       "count                   3267.00     3267.00  \n",
       "mean                      19.08        2.38  \n",
       "std                       14.11        1.41  \n",
       "min                        0.00        0.00  \n",
       "25%                       11.00        2.00  \n",
       "50%                       17.00        2.00  \n",
       "75%                       24.00        3.00  \n",
       "max                      175.00       18.00  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e73d2b-dc30-4a90-983d-200f95635b9f",
   "metadata": {
    "id": "b2e73d2b-dc30-4a90-983d-200f95635b9f"
   },
   "source": [
    "### 2.4 prepare the chunk for the LLM Model\n",
    " now the chunk contains a 10 separated sentences we need to  merge to create a one paragraph to be suitable for LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daf30ff-0aed-4b75-9afd-e6370bdb03fd",
   "metadata": {
    "id": "8daf30ff-0aed-4b75-9afd-e6370bdb03fd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f6496-b4a8-49d1-8bd8-f56d886c6240",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "1156ebdb468145279c591286ba85b95f"
     ]
    },
    "id": "2c8f6496-b4a8-49d1-8bd8-f56d886c6240",
    "outputId": "73329de5-7c4a-4a26-bdc9-c3aedfc1b177"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1156ebdb468145279c591286ba85b95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'sentence_chunks' in item: {'book name': 'Introduction-to-Machine-Learning-with-Python.pdf', 'page_number': 391, 'numberOfCharPerPage': 642, 'numberOfWordPerPage': 110, 'numberOfTokenPerPageExpected': 133.25, 'numberOfSentences': 7, 'text': 'activity near their habitat means greater amounts of sediment and chemicals in the water. In an effort to save this endangered species, biologists have begun to raise the amphibians in captivity and release them when they reach a less vulnerable age. Many of the animals on O’Reilly covers are endangered; all of them are important to the world. To learn more about how you can help, go to animals.oreilly.com. The cover image is from Wood’s Animate Creation. The cover fonts are URW Type‐ writer and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.', 'sentences': ['activity near their habitat means greater amounts of sediment and chemicals in the water.', 'In an effort to save this endangered species, biologists have begun to raise the amphibians in captivity and release them when they reach a less vulnerable age.', 'Many of the animals on O’Reilly covers are endangered; all of them are important to the world.', 'To learn more about how you can help, go to animals.oreilly.com.', 'The cover image is from Wood’s Animate Creation.', 'The cover fonts are URW Type‐ writer and Guardian Sans.', 'The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.'], 'sentences_per_page_spacy': 7, 'sentence_chunks': [['activity near their habitat means greater amounts of sediment and chemicals in the water.', 'In an effort to save this endangered species, biologists have begun to raise the amphibians in captivity and release them when they reach a less vulnerable age.', 'Many of the animals on O’Reilly covers are endangered; all of them are important to the world.', 'To learn more about how you can help, go to animals.oreilly.com.', 'The cover image is from Wood’s Animate Creation.', 'The cover fonts are URW Type‐ writer and Guardian Sans.', 'The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.']], 'num_chunks': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(all_data):\n",
    "   if \"sentence_chunks\" in item:\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"book name\"] = item['book name']\n",
    "        chunk_dict['page_number'] = item['page_number']\n",
    "# join the sentences in one chunk to create a paragragh\n",
    "        join_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \",\" \").strip()\n",
    "        join_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', join_sentence_chunk)\n",
    "        chunk_dict[\"sentence_chunk\"] = join_sentence_chunk\n",
    "\n",
    "        chunk_dict[\"chunk_char_count\"] = len(join_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in join_sentence_chunk.strip(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(join_sentence_chunk)/4\n",
    "\n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "else:\n",
    "     print(f\"Missing 'sentence_chunks' in item: {item}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754b5cb-be6b-4211-af7b-6ec2cc9852d1",
   "metadata": {
    "id": "2754b5cb-be6b-4211-af7b-6ec2cc9852d1"
   },
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df8284-d6ab-4f7f-b2d5-0f17285863b9",
   "metadata": {
    "id": "09df8284-d6ab-4f7f-b2d5-0f17285863b9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd9811-5747-4717-bd20-2bc502df6295",
   "metadata": {
    "id": "43bd9811-5747-4717-bd20-2bc502df6295",
    "outputId": "426ed206-3e17-4f81-ad14-e0f9460ab851"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7780.00</td>\n",
       "      <td>7780.00</td>\n",
       "      <td>7780.00</td>\n",
       "      <td>7780.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>467.39</td>\n",
       "      <td>898.34</td>\n",
       "      <td>898.34</td>\n",
       "      <td>224.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>353.32</td>\n",
       "      <td>517.58</td>\n",
       "      <td>517.58</td>\n",
       "      <td>129.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>184.00</td>\n",
       "      <td>517.00</td>\n",
       "      <td>517.00</td>\n",
       "      <td>129.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>373.00</td>\n",
       "      <td>847.00</td>\n",
       "      <td>847.00</td>\n",
       "      <td>211.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>664.00</td>\n",
       "      <td>1231.00</td>\n",
       "      <td>1231.00</td>\n",
       "      <td>307.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1231.00</td>\n",
       "      <td>10415.00</td>\n",
       "      <td>10415.00</td>\n",
       "      <td>2603.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count      7780.00           7780.00           7780.00            7780.00\n",
       "mean        467.39            898.34            898.34             224.58\n",
       "std         353.32            517.58            517.58             129.39\n",
       "min           0.00              2.00              2.00               0.50\n",
       "25%         184.00            517.00            517.00             129.25\n",
       "50%         373.00            847.00            847.00             211.75\n",
       "75%         664.00           1231.00           1231.00             307.75\n",
       "max        1231.00          10415.00          10415.00            2603.75"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e0863-f283-4d8f-992e-ddb5ceee7814",
   "metadata": {
    "id": "258e0863-f283-4d8f-992e-ddb5ceee7814"
   },
   "source": [
    "### note:\n",
    "The entire textbook has been segmented into manageable chunks, each containing up to 10 sentences, with the corresponding page number recorded.\n",
    "\n",
    "This structured approach enables precise referencing of any text segment, ensuring clear traceability to its original source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3149ec68-f135-453f-be46-1182721cad5d",
   "metadata": {
    "id": "3149ec68-f135-453f-be46-1182721cad5d"
   },
   "source": [
    "### 2.5 Data cleaning\n",
    " #### General Guideline:\n",
    "- < 10 tokens: Likely headers, footers, or non-informative text.\n",
    "\n",
    "- 10–32 tokens: Could be short but valid sentences (e.g., bullet points, definitions).\n",
    "\n",
    "- more than 32 tokens: More likely to contain meaningful and context-rich content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33a5d34-755e-43a9-9563-0452fe4134a4",
   "metadata": {
    "id": "d33a5d34-755e-43a9-9563-0452fe4134a4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45361142-69c8-4dae-9d1e-52ae1b9a77ba",
   "metadata": {
    "id": "45361142-69c8-4dae-9d1e-52ae1b9a77ba",
    "outputId": "21a529ca-86b4-4ff9-b635-7847610a47c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk token count: 0.75 | Text: 602\n",
      "\n",
      "Chunk token count: 1.5 | Text: 23.16)\n",
      "\n",
      "Chunk token count: 10.0 | Text: At the ith iteration, probabilities r at\n",
      "\n",
      "Chunk token count: 2.5 | Text: 1805–1809.\n",
      "\n",
      "Chunk token count: 24.0 | Text: 60] A. K. Jain. Fundamentals of Digital Image Processing. Prentice Hall, Englewood Cliffs, 1989.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_token_length = 32\n",
    "\n",
    "# Print 5 sample chunks that are shorter than or equal to the minimum token threshold\n",
    "for index, row in df4[df4[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
    "    print(f'Chunk token count: {row[\"chunk_token_count\"]} | Text: {row[\"sentence_chunk\"]}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d175b-0cfc-4f0a-9e72-2740d11fcb95",
   "metadata": {
    "id": "5a8d175b-0cfc-4f0a-9e72-2740d11fcb95",
    "outputId": "d5df40c8-cb9e-4bbd-9937-936f268e4e53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'book name': 'Computer-Vision-Algorithms-and-Applications-2nd Edition, Richard Szeliski.pdf',\n",
       "  'page_number': 0,\n",
       "  'sentence_chunk': 'Computer Vision: Algorithms and Applications 2nd Edition Richard Szeliski Final draft, September 30, 2021 © 2022 Springer This electronic draft was downloaded Dec_27,_2022 for the personal use of _________________________???_________________________ ________jacky870810@icloud.com________ and may not be posted or re-distributed in any form. Please refer interested readers to the book’s Web site at https://szeliski.org/Book, where you can also provide feedback.',\n",
       "  'chunk_char_count': 463,\n",
       "  'chunk_word_count': 463,\n",
       "  'chunk_token_count': 115.75},\n",
       " {'book name': 'Computer-Vision-Algorithms-and-Applications-2nd Edition, Richard Szeliski.pdf',\n",
       "  'page_number': 3,\n",
       "  'sentence_chunk': '1 Introduction 1 What is computer vision? •A brief history • Book overview • Sample syllabus • Notation n^ 2 Image formation 33 Geometric primitives and transformations • Photometric image formation • The digital camera 3 Image processing 107 Point operators • Linear ﬁltering • Non-linear ﬁltering • Fourier transforms • Pyramids and wavelets • Geometric transformations 4 Model ﬁtting and optimization 191 Scattered data interpolation • Variational methods and regularization • Markov random ﬁelds 5 Deep learning 235 Supervised learning • Unsupervised learning • Deep neural networks • Convolutional networks • More complex models 6 Recognition 343 Instance recognition • Image classiﬁcation • Object detection • Semantic segmentation • Video understanding • Vision and language 7 Feature detection and matching 417 Points and patches • Edges and contours • Contour tracking • Lines and vanishing points • Segmentation',\n",
       "  'chunk_char_count': 921,\n",
       "  'chunk_word_count': 921,\n",
       "  'chunk_token_count': 230.25}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_chunk= df4[df4[\"chunk_token_count\"]>=min_token_length].to_dict(orient = \"records\")\n",
    "cleaned_chunk[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71505fc1-d83d-4230-a062-bbf5759f4d6c",
   "metadata": {
    "id": "71505fc1-d83d-4230-a062-bbf5759f4d6c"
   },
   "source": [
    "## 2.6 save chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f55b22-ed02-496c-b87f-1228f8150c42",
   "metadata": {
    "id": "c2f55b22-ed02-496c-b87f-1228f8150c42"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.save(cleaned_chunk, \"cleaned_chunk_readyforembedding.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19eacf5-6c9d-4d4b-9024-cf50c7ebafde",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "a19eacf5-6c9d-4d4b-9024-cf50c7ebafde",
    "outputId": "9d560959-9dc2-4909-b563-593ae3e2d9cf"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cleaned_chunk_readyforembedding.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3d1aacc4efe8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcleaned_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cleaned_chunk_readyforembedding.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcleaned_chunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cleaned_chunk_readyforembedding.pt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cleaned_chunk = torch.load(\"cleaned_chunk_readyforembedding.pt\")\n",
    "cleaned_chunk[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef666e5-8ae1-4853-99c4-05ea9e24bfd6",
   "metadata": {
    "id": "eef666e5-8ae1-4853-99c4-05ea9e24bfd6"
   },
   "source": [
    "### 3. Embedding Our Text Chunks\n",
    "While humans interpret and understand text naturally, machines process information most effectively in the form of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0ad9b0-8ccc-4ceb-a48f-6324fb8befe5",
   "metadata": {
    "id": "0d0ad9b0-8ccc-4ceb-a48f-6324fb8befe5"
   },
   "source": [
    "#### Summary of the Process\n",
    "* * Words are meaningless to machines until we map them to numbers.\n",
    "\n",
    "* * Instead of assigning random numbers, we train a model to learn meaning from large text corpora.\n",
    "\n",
    "* * The result: embeddings — vectors that capture semantic meaning.\n",
    "\n",
    "* * Similar words are close together in this vector space.\n",
    "### Why Is This Important?\n",
    "Embedding learning was a major step forward in:\n",
    "\n",
    "- Search engines\n",
    "\n",
    "- Recommendation systems\n",
    "\n",
    "- Chatbots\n",
    "\n",
    "- Translation\n",
    "\n",
    "- Voice assistants\n",
    "\n",
    "- Language generation\n",
    "\n",
    "### Why Are Embeddings Important?\n",
    "* Traditional methods (like one-hot encoding or simple token IDs) treat words as isolated symbols. For example:\n",
    "\n",
    "* \"car\" and \"automobile\" would be completely different and unrelated.\n",
    "\n",
    "But embeddings solve this by placing similar words closer together in vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c074f97-2237-4fd5-a7e9-ca73374f5cbf",
   "metadata": {
    "id": "3c074f97-2237-4fd5-a7e9-ca73374f5cbf"
   },
   "source": [
    "### Why Choose all-mpnet-base-v2?\n",
    "* 📌 1. State-of-the-art accuracy\n",
    "It’s based on Microsoft’s MPNet, which improves over BERT and RoBERTa.\n",
    "\n",
    "Offers strong semantic similarity performance — great for:\n",
    "\n",
    "- Question-answering\n",
    "\n",
    "- Semantic search\n",
    "\n",
    "- Clustering\n",
    "\n",
    "- Duplicate detection\n",
    "\n",
    "* 📌 2. Built for Sentences\n",
    "It’s fine-tuned using Siamese & Triplet networks on sentence pairs.\n",
    "\n",
    "That means embeddings from this model are meaningfully comparable — distances directly reflect semantic similarity.\n",
    "\n",
    "* 📌 3. Efficient\n",
    "Despite being accurate, it's still reasonably fast and lightweight for practical use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a754a6-8de8-4a73-a9de-6f1fdb6c523e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62a754a6-8de8-4a73-a9de-6f1fdb6c523e",
    "outputId": "dcf227f7-4b01-4e84-d28f-3b98fe2a44dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.52.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.32.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.4.26)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m839.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b666e82-719e-44ee-b1a6-983757db20e4",
   "metadata": {
    "id": "3b666e82-719e-44ee-b1a6-983757db20e4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fdc9fc0-3e70-44ab-9422-dc6ab6fa424f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528,
     "referenced_widgets": [
      "5b9f5d427cee4a0b8ba030bfc1998b0a",
      "2068dc852e484850a4c52e6f881f7179",
      "8438f43c47664cabb7999cf08b5c123a",
      "d5753b4a3dd94e63993335392fba4a89",
      "cb5def39363346a18383598751c19c3e",
      "0d3652aaf9204082b969503acb470437",
      "83deb760178f45bdabafe6c2a3777f80",
      "06b309b99bdf4234bba7c30ddc0cb6eb",
      "4d36ba378a674dc0a2be5a5b61973b47",
      "a1b46eb35f264f1aad7421b1fb96eddd",
      "08cfe900f8e8422b968e1d5efc52f4c5",
      "012b42e1db874ae5b60bf66c4b809dc2",
      "bd15dfa2bfb740e2af612d97a35e4d41",
      "de531b0d9ce4439f925bc4fc4b7f11f4",
      "b1b24a239e894f25bc6f6339ebb349a1",
      "8d1522f0969548a79649fa23744b3150",
      "258ea32229ab4dd68d6d809ca5dd289f",
      "116c0c1e7c33409bb7c18444b4d37b23",
      "3e733ff84cf34aa9a38c6a0b5018a8f6",
      "e929895b688940d58a37e680fe9317f6",
      "228f77ba6e9e4609a44cee8c4dc3a4f8",
      "2b05803958df428db0bd1c3ddc9b94c9",
      "894edf8d25db4c849cfdc210ea284b11",
      "8816ffedae134fc8bd875f0110686fa8",
      "2622f3399aa04396bf3e4db986c9fccb",
      "e4e1b682c0784b1bbbb0b0e505547af6",
      "1ef32d2c07954937b908fb4c443eb0fa",
      "90de78048ded4cc0b5815646ef59321b",
      "341a1dce73aa428abe2dfd6953c60416",
      "e68d202db4f84ce793b9f28919d896fc",
      "24583aeb910c42c4b169d313b4d944d2",
      "c7c9d9347e2644489e5382c1b21dbfb9",
      "d889efea957d42118191b36cf5286ab7",
      "bac98533918341ccafa67003af37a863",
      "ded4e2aa95b54729afc97412b5d038e2",
      "a28843f53832478f91b8dc8aa01f59e6",
      "6da185383f1f472aac7ab1cec3109d6c",
      "b5ce84265c4445d6a1f7e6e02dee3f1a",
      "0f68220c98c94162b06ca5d41f78e016",
      "8d77932b359546c6a7551364384bf7ac",
      "5b4a4ef6b46c46d1ab2ff37f6dee997b",
      "6d2f3845cbc348c188bafdba078f565e",
      "b5ab5f80b6f34d669718ab9ebdacab01",
      "13550f0f42b446aabaa77421f5b80b5e",
      "3ae1d72026364e1baa772cdec87babf8",
      "e840eb5995c64d2e8b74de2681c30c98",
      "ca601e2238274006928ed80e0857e971",
      "a4917d41a1ac440c9e516ce172505c5b",
      "9c169137e23e4945b14f5ec05c5a3b65",
      "ddd7c3f793064c339daea17cdad42230",
      "bb6fb8d51423421dae495652e57278a4",
      "78df3853d294439baac2ad70e643a980",
      "f06a6e8f94c64a038d69f76411edfad6",
      "664fb5138522485ba4bee2664c54e744",
      "cf5e0219412f4bd495f56355b3a7393d",
      "1e1e8e2033644185bd5aae4581e8bdb1",
      "761591167ed5441fb8b98b156abe19f9",
      "0386bb2b9e2e4157a866cd8400de3eaf",
      "dc2ae5b72bc74230a2588e5d27812aad",
      "b1a429a971e8489f98c8dddf1dc57e74",
      "def2fc6df48045a19cd188706b3526eb",
      "bae11872bc69410fb3f7d1c7b0cb79c7",
      "f60dd4e116e84a2f93ff9d164cbbcca7",
      "2a18cd691cfd44e9b1991576266073ab",
      "46853c06b0a94c5ea1e95b531fb32c79",
      "f503058a0b6e49679d243b29fc403046",
      "002fd4115e6f44d6aba2135918f80622",
      "fd36ef3328e44c5c8ebedb2f9c827fd9",
      "d14abba5de9849edaf56dd7ae4895d90",
      "f75f21360fa04f5aa480f0545aee5262",
      "147ed6fadfe94916bb8a651ee09c1604",
      "ecb468b56f3f408f971452587eb8ace8",
      "3b7ed7fc91364d10b516e7a323cf0165",
      "d261ae0aa1dc4234821aeff7b1532c06",
      "7ffa10e14a344872b530dba40016610e",
      "796499dac0e049afab88318af52054de",
      "97d7402588d14174a5f1ed4f874668a9",
      "38f96236861d4457b0bbc6f1ce8910de",
      "01b5d4bc76ac47c69a291dfc866ab6c0",
      "76e1e70c41544390ba26bd8603b250fc",
      "60b58113e6b14edc9fd8f34227066e4a",
      "c9dad9234d1e4ce58a5c4036472b8c54",
      "a61fb91584dc46869d7d64e9b92c01ad",
      "2d6a211ad54c4c83904cf787099ec6c5",
      "f7db3598c58643a29995073501f22cfd",
      "d21e5ccab6144967a3d229816ea75d7c",
      "133502aa71e14939af3fbc16b89a5800",
      "5bf13f20de3e407f82b73308d6a2adfb",
      "543d6a67a413412f9b1f5bed6cbe9f48",
      "fb93c39cc5de4ee798868ab9085ebd2b",
      "1d83bebd227c4b5b97f9ba61c3cf81ba",
      "28b38869e2aa40798f9f7699588e457d",
      "55fae7a3bf9b48f6a6ce60ad0fa6875d",
      "d2fdb8649a654482aa72b4bc8b4661ff",
      "83ce27e3da0e419fb572cc30cb9e6edc",
      "7b38aac023914ac38a71b2914a9b42e6",
      "b2b35581726c4ad3bdc9cef6c3eb7e34",
      "b023b9281dac47efa9ddc513495f6672",
      "0baaa2dafd424f30bfa3477f0b6023db",
      "325831aa847946d99eec509c9074242a",
      "f14904142ec04bda99f5b29e9a994ae8",
      "500d324293524a6dbc3c0959533a6272",
      "3c8784a9d3e3453294a756c0943d9155",
      "8f82c433465c420ab52a33d49d6bd398",
      "57b782483d9149fc99a81d78fc2cfce0",
      "77f32c38e25543f9baf7f569c44eb045",
      "f1cfc5b3bcfd40ed97bfdf9b3a39629e",
      "21bf256c040140ddaa54aeadc089c6ba",
      "0b14da34181d4a4e8fc9902da725278a",
      "a3900c43cdb64c9695ec2c623fcb00b6",
      "daa4600ec3bf48909e06cd9f7a98b593",
      "96c4e1ef54554fcbacd1d6ba9efb92f9",
      "befc260af1c849b8b2be86c74c483273",
      "82909ffa9bee4ec78143d67f97f25c52",
      "dacdc31bfd94461db720adf8e007abdf",
      "47ff4e7fae6345b78f1ebf2d486c58fc",
      "1ee91bbc04814775b29cec52b2b57e2c",
      "9098a890790741839ddcb119537531de",
      "b641acfa395f46769c7b81dc3d869fe2",
      "412fc9c59d9940b29d27eac8abcf3efa",
      "19ef372927024f8f81ad61023d842408"
     ]
    },
    "id": "1fdc9fc0-3e70-44ab-9422-dc6ab6fa424f",
    "outputId": "665da2da-9363-4a19-c5f8-611b217e43b8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9f5d427cee4a0b8ba030bfc1998b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012b42e1db874ae5b60bf66c4b809dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894edf8d25db4c849cfdc210ea284b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac98533918341ccafa67003af37a863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae1d72026364e1baa772cdec87babf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1e8e2033644185bd5aae4581e8bdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002fd4115e6f44d6aba2135918f80622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f96236861d4457b0bbc6f1ce8910de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543d6a67a413412f9b1f5bed6cbe9f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325831aa847946d99eec509c9074242a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa4600ec3bf48909e06cd9f7a98b593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.06 s, sys: 1.18 s, total: 5.24 s\n",
      "Wall time: 9.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sentence_transformers import SentenceTransformer , util\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
    "                                      device=\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4dbe07-adba-44db-bc76-d976dbc092d5",
   "metadata": {
    "id": "0c4dbe07-adba-44db-bc76-d976dbc092d5"
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Extract all sentences\n",
    "# sentences = [item[\"sentence_chunk\"] for item in cleaned_chunk]\n",
    "\n",
    "# # Batch encode\n",
    "# embeddings = embedding_model.encode(sentences, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# # Attach embeddings back to items\n",
    "# for item, emb in zip(cleaned_chunk, embeddings):\n",
    "#     item['embedding'] = emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfde2a0-33c6-4a69-9b95-284f808f66e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "61bca3a702b44a20822dc32ef2aacbde",
      "65810c694b2b449cad34439b83b7849d",
      "1dbfcd08f5eb4e8e8676993465f269d8",
      "b280d96d958f439cbaae8bcf69951b00",
      "f7a211e6fe644846939706d3674bdae5",
      "4ffe987c171a4a93bfe04f11bcddbb7a",
      "f38e95544a0d477db6b14326e1b8bff7",
      "86a22d0fabf14598aee377dded21447b",
      "7b53f282461f46948f8c750c8c569411",
      "250b39eaeab14335825ab855f4302a52",
      "9b3886864e6d4f7798aca77db3e1ad14"
     ]
    },
    "id": "fcfde2a0-33c6-4a69-9b95-284f808f66e7",
    "outputId": "06054210-0398-41dd-a0e3-2cd4cefb4094"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bca3a702b44a20822dc32ef2aacbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7475 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for item in tqdm(cleaned_chunk):\n",
    "    item['embedding'] = embedding_model.encode(item[\"sentence_chunk\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c3b98-d345-4d74-969f-7613604ea0c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de3c3b98-d345-4d74-969f-7613604ea0c5",
    "outputId": "5a10cc35-d38f-4d1c-df6a-d33bd8625ee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.save(cleaned_chunk,\"cleaned_embedded_chunked2.pt\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea13aeb-018e-4a6c-8c26-59a844e99b72",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ea13aeb-018e-4a6c-8c26-59a844e99b72",
    "outputId": "b393107c-71ef-4998-e854-6b95e78d9378"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-c637fb0312ea>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cleaned_chunk= torch.load(\"cleaned_embedded_chunked2.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cleaned_chunk= torch.load(\"cleaned_embedded_chunked2.pt\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd86a9-5871-49c1-86ed-11d2fc82a30b",
   "metadata": {
    "id": "b6cd86a9-5871-49c1-86ed-11d2fc82a30b"
   },
   "outputs": [],
   "source": [
    "# loaded_cleaned = torch.load(\"cleaned_chunk.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b22db7-0747-47d5-89d8-af428c2fa1c3",
   "metadata": {
    "id": "d1b22db7-0747-47d5-89d8-af428c2fa1c3"
   },
   "outputs": [],
   "source": [
    "# list_of_chunk = [item[\"sentence_chunk\"] for item in cleaned_chunk]\n",
    "# list_of_chunk[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca4f12-548a-4ae0-a956-cee024a97355",
   "metadata": {
    "id": "42ca4f12-548a-4ae0-a956-cee024a97355"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# chuncked_text_embedding = embedding_model.encode(list_of_chuck ,\n",
    "#                                                  batch_size=32 , convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb2a83c-cdca-4f4a-976c-dacede2a4fac",
   "metadata": {
    "id": "ddb2a83c-cdca-4f4a-976c-dacede2a4fac"
   },
   "source": [
    "### Extract and creat list of embedding from the ccleaned_chunk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "977e773d-bd55-4fe3-8cec-be491d5276f9",
   "metadata": {
    "id": "977e773d-bd55-4fe3-8cec-be491d5276f9"
   },
   "outputs": [],
   "source": [
    "embedding = [item[\"embedding\"]  for item in cleaned_chunk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60ec2aa-dfcf-4d45-b7dd-47a0995b16c0",
   "metadata": {
    "id": "b60ec2aa-dfcf-4d45-b7dd-47a0995b16c0",
    "outputId": "99b91ac8-07f7-4951-d92d-1adf9ce98d6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.22471596e-02,  1.00584119e-04, -1.83501712e-03,  5.88923395e-02,\n",
       "       -1.45165594e-02, -7.36929197e-03,  7.02884980e-03,  4.17751782e-02,\n",
       "        8.59931391e-03,  1.62962321e-02,  5.32661937e-02,  4.79736514e-02,\n",
       "        5.10932803e-02, -6.71845209e-03, -2.33528372e-02, -7.21996352e-02,\n",
       "       -6.19428121e-02, -1.44209685e-02, -4.70060110e-02, -1.91253610e-02,\n",
       "        1.12584075e-02, -2.87137274e-02,  6.10022293e-03, -1.22142897e-03,\n",
       "       -8.42429325e-02,  2.82193366e-02, -5.27978968e-03, -2.68212110e-02,\n",
       "        4.57868204e-02,  1.06517570e-02, -2.46156752e-02,  1.27255861e-02,\n",
       "       -8.83830525e-03,  1.29647609e-02,  2.11044767e-06, -7.65771642e-02,\n",
       "       -7.20729725e-03, -1.42689813e-02,  7.50446618e-02, -7.20035518e-03,\n",
       "        3.30601446e-02,  4.91423649e-04,  1.63715705e-02, -3.68691608e-02,\n",
       "       -2.09438540e-02,  2.77584977e-02,  6.08712770e-02,  1.47208916e-02,\n",
       "        1.60253011e-02,  1.52521050e-02, -2.29642931e-02,  4.95672401e-04,\n",
       "       -2.35068444e-02,  2.95391064e-02, -3.88785708e-03, -1.44677339e-02,\n",
       "       -3.16242105e-03, -7.22619193e-03, -2.68044025e-02, -5.49308471e-02,\n",
       "       -1.49507821e-02,  8.22555274e-02,  8.60957336e-03, -3.42569351e-02,\n",
       "        7.02433959e-02,  1.62692852e-02, -9.30767972e-03, -3.76423970e-02,\n",
       "       -6.75694644e-02,  1.76673047e-02, -5.75119928e-02, -2.81634601e-03,\n",
       "        1.50708398e-02, -3.84083227e-03, -4.39077243e-02,  2.54053734e-02,\n",
       "        1.63742397e-02,  9.26784147e-03, -1.01788612e-02, -2.40318775e-02,\n",
       "       -9.23856422e-02,  1.96390320e-02, -2.75071827e-04, -9.12694633e-03,\n",
       "       -4.90688533e-02,  1.72127914e-02,  2.90564187e-02,  4.70156372e-02,\n",
       "        5.02855238e-03, -1.93488933e-02, -1.92183605e-03, -3.96668836e-02,\n",
       "        3.97989973e-02,  3.88715602e-02, -7.64915906e-03, -5.20184040e-02,\n",
       "       -3.87116498e-03, -4.50536348e-02,  3.40472930e-03, -2.68518459e-02,\n",
       "        7.91849792e-02,  1.38222324e-02,  1.74397100e-02,  4.08910364e-02,\n",
       "       -7.12351874e-02,  2.29718536e-02, -2.86733825e-02,  1.40217664e-02,\n",
       "       -6.91040382e-02,  2.90987752e-02,  4.37810784e-03,  2.48561762e-02,\n",
       "       -5.96244112e-02, -3.08712642e-03,  3.54224332e-02,  1.09897358e-02,\n",
       "       -1.81502085e-02, -1.96069293e-02, -8.83579778e-04, -3.49101089e-02,\n",
       "       -1.37108611e-03, -3.52553878e-04,  3.49309295e-02, -3.88364270e-02,\n",
       "       -7.64444529e-04,  8.74262676e-03, -1.88973844e-02,  1.06032044e-02,\n",
       "       -5.52998595e-02,  3.90164033e-02,  1.55121107e-02,  2.25551128e-02,\n",
       "        3.00999135e-02,  2.18592063e-02,  1.11358361e-02,  1.85339376e-02,\n",
       "        1.30008906e-02, -2.54780985e-02, -4.16700132e-02,  2.86659058e-02,\n",
       "        9.40673519e-03, -2.59270072e-02,  4.51120827e-03, -2.48086471e-02,\n",
       "        1.79493818e-02, -5.92525154e-02,  3.74609008e-02,  4.36132923e-02,\n",
       "        1.81322601e-02, -2.87145637e-02,  2.50422116e-02,  4.58844230e-02,\n",
       "       -4.13195975e-02,  1.85122155e-02,  5.42308278e-02,  2.05499977e-02,\n",
       "       -3.40835601e-02,  1.33684007e-02,  3.17404494e-02,  1.34680839e-02,\n",
       "        3.55498865e-02, -1.83646735e-02,  3.80679332e-02,  3.62235084e-02,\n",
       "       -4.85533178e-02,  5.61305694e-03, -1.05522701e-03, -5.65822572e-02,\n",
       "        2.82844715e-02, -6.37225956e-02, -1.60240624e-02, -1.98671576e-02,\n",
       "        2.22519487e-02, -3.21011879e-02,  2.75640637e-02,  4.40891981e-02,\n",
       "        2.53766570e-02, -3.61126587e-02,  1.15247164e-02, -1.62822451e-03,\n",
       "        1.24017121e-02, -4.22503576e-02,  5.94965070e-02,  3.09172235e-02,\n",
       "       -7.41556883e-02, -4.98143025e-02, -8.08066502e-02,  5.83927557e-02,\n",
       "       -5.56260198e-02, -3.85707691e-02, -1.94165278e-02, -3.12241614e-02,\n",
       "       -9.10660625e-03,  9.66989249e-03,  8.62786546e-02, -5.95419891e-02,\n",
       "       -2.73859478e-03, -6.84786886e-02, -3.94072477e-03, -6.99056638e-03,\n",
       "       -2.71346718e-02, -6.40724301e-02,  1.96738467e-02, -2.87930500e-02,\n",
       "       -1.82910897e-02, -5.29742949e-02,  2.77014561e-02,  4.99418192e-02,\n",
       "       -3.49616371e-02, -6.10555708e-02,  1.60446612e-03,  2.43472196e-02,\n",
       "        5.91403544e-02, -1.18927872e-02, -3.37376893e-02,  1.09836400e-01,\n",
       "        1.73078794e-02,  8.32361355e-03, -5.32457232e-03,  3.22572365e-02,\n",
       "        3.70574705e-02,  2.48523932e-02,  2.24420819e-02,  3.08491644e-02,\n",
       "        3.02272364e-02,  5.58976084e-02,  3.90994400e-02,  2.93969605e-02,\n",
       "        1.19191911e-02,  5.27694523e-02,  6.86031953e-02, -1.80976801e-02,\n",
       "        4.85431962e-02,  6.19800820e-04,  2.97626927e-02,  1.97711419e-02,\n",
       "        7.61678442e-02,  8.16505477e-02,  4.81630005e-02, -9.31338817e-02,\n",
       "        2.28147022e-02,  2.86218580e-02,  1.37606999e-02, -4.96574119e-02,\n",
       "        1.04738763e-02, -6.81603029e-02, -2.90576480e-02, -3.97869609e-02,\n",
       "        1.28149269e-02, -3.28631327e-02, -4.11115959e-02, -2.88610696e-03,\n",
       "        2.33786106e-02,  3.51816230e-02,  1.53472451e-02, -1.29486155e-02,\n",
       "       -1.32617578e-02,  1.10528124e-02, -3.01871449e-02,  1.57107301e-02,\n",
       "       -3.33376117e-02, -5.83643727e-02, -2.00178828e-02,  3.90110053e-02,\n",
       "        3.20182592e-02,  1.61252487e-02,  1.36121605e-02,  1.70723088e-02,\n",
       "       -5.86704277e-02, -2.11287336e-03, -5.60405068e-02, -2.75915465e-03,\n",
       "       -3.19201760e-02,  1.84550788e-02,  3.44366580e-02,  5.91796525e-02,\n",
       "        7.28722364e-02,  3.10793705e-02, -6.88063577e-02, -1.99643709e-02,\n",
       "       -2.83323713e-02, -8.06431323e-02,  2.12570187e-02, -2.06795856e-02,\n",
       "        3.39538977e-03,  2.06614528e-02,  5.69691658e-02, -9.48912129e-02,\n",
       "       -4.34558466e-02,  1.84291229e-03, -1.61146023e-03, -2.11904738e-02,\n",
       "       -8.35319050e-03,  7.35754007e-03, -1.24364682e-02, -2.15781853e-02,\n",
       "       -2.17506476e-02,  9.31561291e-02, -3.20967869e-03,  5.37170842e-03,\n",
       "        5.03080189e-02,  8.75427127e-02,  1.25024877e-02,  1.22569746e-03,\n",
       "       -2.92365011e-02,  4.16542068e-02,  3.92620862e-02, -5.03088720e-02,\n",
       "        3.16189253e-04, -3.95458341e-02, -2.02268977e-02,  2.13546958e-02,\n",
       "       -3.22729233e-03,  9.46040079e-03, -7.11566629e-03, -4.27506072e-03,\n",
       "       -4.98207584e-02, -9.48828738e-03,  1.12244040e-02,  4.84934784e-02,\n",
       "        2.66866898e-03, -8.14371277e-03, -3.11825238e-02, -1.87755376e-02,\n",
       "        3.79993841e-02,  3.88446078e-02, -6.83024451e-02,  2.46285275e-02,\n",
       "       -1.44583695e-02,  4.09227386e-02,  2.55229343e-02, -9.54740867e-03,\n",
       "        3.81130576e-02,  1.61321871e-02, -1.36500690e-02,  4.32335325e-02,\n",
       "       -3.11488975e-02, -1.68168377e-02, -5.47704175e-02,  1.89773850e-02,\n",
       "        3.25658396e-02, -5.98873664e-03, -2.44599562e-02, -5.70426648e-03,\n",
       "       -1.14981283e-03, -4.05553393e-02, -3.23035233e-02, -5.95267341e-02,\n",
       "       -9.03385039e-03,  1.99433453e-02,  1.19896876e-02,  1.73242539e-02,\n",
       "        4.24311683e-02, -4.21606842e-03, -7.29317684e-03,  7.82752186e-02,\n",
       "       -1.08573074e-02, -4.49270494e-02, -1.41592175e-02, -1.18557028e-02,\n",
       "        2.60260422e-03,  6.60155937e-02,  1.86072439e-02,  6.09767213e-02,\n",
       "       -9.78413895e-02,  1.04968913e-03,  2.34065391e-02,  5.33035100e-02,\n",
       "        1.38221513e-02, -1.76019426e-02,  4.59878668e-02,  6.11511730e-02,\n",
       "       -3.07870042e-02,  1.79510918e-02, -2.14720722e-02, -8.54112431e-02,\n",
       "       -1.50938444e-02,  2.40760297e-02, -5.95584027e-02,  1.33008147e-02,\n",
       "        2.09165439e-02,  2.34939158e-02,  3.03625856e-02,  2.92245932e-02,\n",
       "       -4.09285985e-02, -1.54320803e-02,  2.81911343e-02,  2.59468742e-02,\n",
       "       -9.00590699e-03,  1.45687014e-02,  3.66833955e-02, -2.23750919e-02,\n",
       "       -5.79056442e-02, -5.94139881e-02,  1.26444791e-02, -3.16177607e-02,\n",
       "       -8.52720439e-02, -1.43568357e-02,  4.16462123e-03, -5.73377907e-02,\n",
       "       -6.63636299e-03, -8.96082446e-03,  4.56506796e-02, -1.24732377e-02,\n",
       "       -5.65560609e-02, -6.77227676e-02,  1.03854192e-02,  1.98165588e-02,\n",
       "       -4.23997734e-03, -6.64856955e-02, -8.41256045e-03, -3.28098498e-02,\n",
       "       -6.91269711e-03, -4.41143997e-02, -1.90382209e-02,  6.12120964e-02,\n",
       "        3.20325606e-02, -2.69259475e-02,  4.85153571e-02,  3.25155146e-02,\n",
       "       -1.34135550e-02, -1.33520854e-03,  3.65309455e-02, -6.29692804e-03,\n",
       "       -1.89462937e-02, -6.65019825e-02,  1.26937078e-02,  1.70521308e-02,\n",
       "       -5.10578684e-04, -1.60869285e-02, -3.28942798e-02, -7.34373182e-03,\n",
       "        4.04235274e-02,  6.02680817e-02, -4.37929016e-03, -4.65673453e-04,\n",
       "        1.86837073e-02, -8.28677323e-04, -3.13592441e-02, -2.28461493e-02,\n",
       "        4.80334088e-02,  3.04922629e-02,  1.05370268e-01,  5.27409604e-03,\n",
       "       -6.86589777e-02,  1.41983815e-02,  4.61335815e-02, -6.90740999e-03,\n",
       "        1.74154956e-02,  1.89007241e-02,  2.60057067e-03,  2.03078683e-03,\n",
       "        4.05810215e-02, -1.06993336e-02, -2.06011180e-02, -3.10369972e-02,\n",
       "        9.08974372e-03, -4.98157255e-02, -9.76007245e-03,  3.79661322e-02,\n",
       "        9.81969684e-02,  3.12418565e-02, -7.79850502e-03, -6.01275116e-02,\n",
       "       -6.59432495e-03,  2.74246093e-02, -2.15809960e-02,  6.30415604e-02,\n",
       "        2.01792251e-02, -3.33286077e-03, -1.94856282e-02, -5.14696799e-02,\n",
       "        2.79510133e-02,  1.11794677e-02,  5.15757836e-02,  2.51774509e-02,\n",
       "       -6.77675903e-02, -4.11941148e-02, -4.09744456e-02, -1.06216120e-02,\n",
       "       -3.61724272e-02,  6.26673996e-02,  2.01255381e-02,  8.57293420e-03,\n",
       "       -1.86811034e-02,  2.70783976e-02,  2.07241066e-02, -1.59870014e-02,\n",
       "       -2.63039325e-03, -3.56695689e-02, -1.87076107e-02,  8.25207774e-03,\n",
       "       -2.52857599e-02, -4.65547070e-02,  1.36469072e-02, -1.46404374e-02,\n",
       "       -2.40533315e-02, -2.50270087e-02,  4.31481414e-02, -2.44631600e-02,\n",
       "       -5.78084998e-02,  3.06142215e-03,  5.06426133e-02,  4.82178740e-02,\n",
       "       -1.96085242e-03,  9.81920026e-03,  5.71316592e-02, -8.79997760e-03,\n",
       "        3.61507535e-02, -4.02024761e-02, -1.73587762e-02, -3.58143747e-02,\n",
       "        1.45654371e-02,  2.03859620e-03, -4.27754410e-02,  1.94696579e-02,\n",
       "        7.25664198e-02, -5.13982922e-02, -2.09286762e-03, -5.08779585e-02,\n",
       "       -1.21106384e-02, -5.29067740e-02,  3.83030139e-02,  1.89486239e-02,\n",
       "        4.52999119e-03,  1.92061309e-02,  1.98754519e-02, -1.49671012e-03,\n",
       "        2.09373925e-02, -2.87463460e-02, -9.50888079e-03, -3.42863575e-02,\n",
       "        2.48060189e-02, -5.78999221e-02, -3.05748116e-02,  4.33521671e-03,\n",
       "        2.60620695e-02, -1.77462604e-02, -3.78865041e-02,  3.27416845e-02,\n",
       "       -4.86412346e-02,  9.28247720e-03,  3.35109606e-02, -1.57005228e-02,\n",
       "        7.91823398e-03,  2.60872394e-02,  2.23849006e-02, -8.61919951e-03,\n",
       "       -3.21096182e-03, -1.28920730e-02,  2.30850559e-02, -3.90441716e-02,\n",
       "       -5.18707633e-02,  5.01070544e-03, -6.07108092e-03, -6.09951235e-33,\n",
       "        2.23571770e-02, -1.20251656e-01, -8.61917287e-02,  1.23031856e-02,\n",
       "       -4.11694944e-02,  2.49537155e-02,  2.53863465e-02, -6.43092021e-03,\n",
       "       -2.09430866e-02, -1.77499896e-03, -2.78517394e-03,  4.55856100e-02,\n",
       "        2.28841286e-02, -6.71417080e-03, -3.77996080e-02,  3.91003937e-02,\n",
       "       -2.86270604e-02,  6.40174886e-03, -2.65477113e-02, -5.34126274e-02,\n",
       "        5.27201872e-03,  3.29577029e-02,  1.73273571e-02, -5.98058850e-02,\n",
       "       -6.38057068e-02,  5.85182104e-03, -6.63951263e-02,  2.83398870e-02,\n",
       "       -5.09442762e-02, -1.08369801e-03, -7.46390224e-03, -2.69390494e-02,\n",
       "        5.17214416e-03,  2.81459726e-02,  4.50556772e-03,  1.72713138e-02,\n",
       "       -2.13534087e-02, -5.29734716e-02, -7.37231821e-02,  8.03370327e-02,\n",
       "       -2.21289974e-02,  3.05862376e-03,  6.83620945e-02, -4.39985245e-02,\n",
       "       -8.72744396e-02, -7.46255070e-02,  2.86416262e-02, -1.07549131e-02,\n",
       "        1.09122798e-03,  2.99100094e-02,  3.85162450e-04,  1.05884625e-02,\n",
       "       -1.57541744e-02, -3.53781842e-02,  3.02105527e-02, -2.41980106e-02,\n",
       "        2.16135401e-02, -2.45051533e-02, -4.58359579e-03,  4.01141942e-02,\n",
       "        6.54259406e-04,  3.40012535e-02,  2.83251759e-02,  3.08133289e-02,\n",
       "        8.39657988e-03,  4.72347178e-02,  4.96090874e-02, -8.89929086e-02,\n",
       "        4.20263261e-02, -5.79507425e-02, -3.36035900e-03,  3.72773036e-02,\n",
       "        1.67262182e-02, -1.91341005e-02,  5.55861406e-02,  3.41420551e-03,\n",
       "       -2.10716538e-02,  1.31746230e-03, -4.35495004e-02,  5.72069362e-02,\n",
       "       -2.25532316e-02, -2.28589121e-02, -2.35307869e-02, -7.32895266e-03,\n",
       "        4.22678329e-02, -4.82137389e-02,  2.72592437e-03,  1.28876278e-02,\n",
       "       -3.50086056e-02,  2.46757362e-02, -5.25798462e-02,  4.85654585e-02,\n",
       "       -1.46648595e-02,  1.89882182e-02,  3.56067158e-02, -6.54507205e-02,\n",
       "        5.38941398e-02,  5.01392037e-02,  3.29300836e-02,  3.64551693e-02,\n",
       "        3.22816917e-03, -4.19276059e-02, -1.87438559e-02, -3.00319889e-03,\n",
       "        2.92531401e-02, -2.17104256e-02, -2.83779372e-02,  5.12678511e-02,\n",
       "        1.54457130e-02, -1.17227342e-02,  1.52994720e-02, -6.91691190e-02,\n",
       "       -1.75169029e-04,  1.21776611e-02,  5.78299798e-02,  8.10248684e-03,\n",
       "        9.29609034e-03,  6.13005273e-02, -1.69649366e-02, -6.41260147e-02,\n",
       "        2.66014189e-02, -1.82661638e-02,  1.34345738e-03,  3.19955982e-02,\n",
       "       -3.77941728e-02,  8.66029691e-03, -1.86469387e-02, -4.98969890e-02,\n",
       "       -1.00668129e-02,  6.58937031e-03, -5.37779406e-02, -6.20425120e-02,\n",
       "        2.75728411e-07, -7.23409606e-03,  2.57476922e-02,  1.17183262e-02,\n",
       "       -1.96363460e-02, -1.22394115e-02, -2.12155282e-02, -4.81915586e-02,\n",
       "        5.25569618e-02, -4.62799855e-02, -2.48037223e-02,  7.76076037e-03,\n",
       "       -6.44509727e-03, -5.50887175e-03, -3.46163549e-02, -1.17595168e-02,\n",
       "        4.56930883e-02,  5.86895011e-02,  1.78040676e-02, -4.63637710e-02,\n",
       "        4.89331037e-02,  2.62633637e-02,  9.19802580e-03,  2.37352364e-02,\n",
       "       -3.05561237e-02,  1.73336349e-03,  2.14721281e-02,  3.99890766e-02,\n",
       "       -2.80314889e-02,  2.17332896e-02, -3.66262905e-02,  1.52051738e-02,\n",
       "       -8.70148279e-03, -1.37450872e-03,  2.90732756e-02, -6.28404459e-03,\n",
       "        3.63913924e-02,  6.33365512e-02,  2.98235193e-02, -2.50929277e-02,\n",
       "        6.95954412e-02, -1.00408774e-02,  9.20176357e-02,  3.04774102e-02,\n",
       "       -4.67141159e-03,  5.50591759e-02,  3.94822732e-02,  5.65639418e-03,\n",
       "       -1.17166871e-02, -5.58119966e-03, -1.59225042e-03,  4.28122878e-02,\n",
       "        1.79872084e-02,  1.55019155e-02,  2.87548210e-02, -2.33150162e-02,\n",
       "       -1.76398959e-02,  1.22437216e-02, -9.80090350e-03,  2.51738653e-02,\n",
       "       -2.67966259e-02, -6.45917701e-03, -4.25888179e-03,  3.33044529e-02,\n",
       "        2.95898262e-02,  8.71237591e-02, -3.93093862e-02, -2.44973060e-02,\n",
       "        2.20058420e-34, -6.43097162e-02,  1.00604221e-02, -1.22028096e-02,\n",
       "        6.30331272e-03, -4.19145450e-02,  2.29011849e-02, -6.13952894e-03,\n",
       "        6.05838280e-03,  2.90714446e-02,  9.06571653e-03,  1.85460644e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f28fed9-57e6-4aca-9115-c581da247887",
   "metadata": {
    "id": "0f28fed9-57e6-4aca-9115-c581da247887"
   },
   "source": [
    "### 4.0 Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009af73d-4e5d-4b11-9a86-cfeea6b8ee31",
   "metadata": {
    "id": "009af73d-4e5d-4b11-9a86-cfeea6b8ee31"
   },
   "source": [
    "### 4. Pass the search query to the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b9251c-0959-44c4-8c66-86e317d2d4cb",
   "metadata": {
    "id": "61b9251c-0959-44c4-8c66-86e317d2d4cb"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer , util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4ae7183-aac0-40fb-b689-51921fa53218",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4ae7183-aac0-40fb-b689-51921fa53218",
    "outputId": "8edbf53c-022b-46cf-a748-214880d62edb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding length : 7475\n",
      "CPU times: user 1.03 s, sys: 188 ms, total: 1.22 s\n",
      "Wall time: 1.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([1.8159, 1.8129, 1.7371, 1.6693, 1.6620], device='cuda:0'),\n",
       "indices=tensor([4871, 4872, 5017, 4943, 4653], device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 1. Define the search Query\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "query = \"How can infrastructure help reduce engineering effort in ML deployment?\"\n",
    "\n",
    "# 2. Embed the search query to the same model\n",
    "query_embedded = embedding_model.encode(query , convert_to_tensor=True)\n",
    "# 3. find the similarity in the given text using dot product\n",
    "embeddings_tensor = torch.tensor(embedding)\n",
    "\n",
    "a = query_embedded.to(device)\n",
    "b = embeddings_tensor.to(device)\n",
    "\n",
    "dot_score = util.dot_score(a=a , b=b)[0]\n",
    "\n",
    "print(f\"embedding length : {len(embeddings_tensor)}\")\n",
    "top_search_result_dot_product =torch.topk(dot_score, k=5)\n",
    "\n",
    "\n",
    "top_search_result_dot_product\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8afca61c-46d6-454b-acc4-a7340d22a0e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "8afca61c-46d6-454b-acc4-a7340d22a0e9",
    "outputId": "2fff8731-f478-40a0-d110-1abef3c01082"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'• What question(s) am I trying to answer?Do I think the data collected can answer that question? •What is the best way to phrase my question(s) as a machine learning problem? •Have I collected enough data to represent the problem I want to solve? •What features of the data did I extract, and will these enable the right predictions? •How will I measure success in my application? •How will the machine learning solution interact with other parts of my research or business product?In a larger context, the algorithms and methods in machine learning are only one part of a greater process to solve a particular problem, and it is good to keep the big picture in mind at all times. Many people spend a lot of time building complex machine learning solutions, only to find out they don’t solve the right problem. When going deep into the technical aspects of machine learning (as we will in this book), it is easy to lose sight of the ultimate goals.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_chunk[6958]['sentence_chunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16bcc9ba-b85d-4f34-b04d-4b4aa28f5fac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16bcc9ba-b85d-4f34-b04d-4b4aa28f5fac",
    "outputId": "be94fb75-095c-4504-debb-89fae6f6ce81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 10. Infrastructure and Tooling for MLOps In Chapters 4 to 6, we discussed the logic for developing ML systems. In Chapters 7 to 9, we discussed the considerations for deploying, monitoring, and continually updating an ML system. Up until now, we’ve assumed that ML practitioners have access to all the tools and infrastructure they need to implement that logic and carry out these considerations. However, that assumption is far from being true. Many data scientists have told me that they know the right things to do for their ML systems, but they can’t do them because their infrastructure isn’t set up in a way that enables them to do so. ML systems are complex. The more complex a system, the more it can benefit from good infrastructure. Infrastructure, when set up right, can help automate processes, reducing the need for specialized knowledge and engineering time. This, in turn, can speed up the development and delivery of ML applications, reduce the surface area for bugs, and enable new use cases.\n",
      " -When set up wrong, however, infrastructure is painful to use and expensive to replace. In this chapter, we’ll discuss how to set up infrastructure right for ML systems. Before we dive in, it’s important to note that every company’s infrastructure needs are different. The infrastructure required for you depends on the number of applications you develop and how specialized the applications are. At one end of the spectrum, you have companies that use ML for ad hoc business analytics such as to project the number of new users they’ll have next year to present at their quarterly planning meeting. These companies probably won’t need to invest in any infrastructure—Jupyter Notebooks, Python, and Pandas would be their best friends. If you have only one simple ML use case, such as an Android app for object detection to show your friends, you probably won’t need any infrastructure either—you just need an Android-compatible ML framework like TensorFlow Lite. At the other end of the spectrum, there are companies that work on applications with unique requirements. For example, self-driving cars have unique accuracy\n",
      " -scale, Myth 4: Most ML Engineers Don’t Need to Worry About Scale separation of responsibilities, Model Deployment and Prediction Service shadow deployment, Shadow Deployment development environment, infrastructure, Infrastructure and Tooling for MLOps, Development Environment containers, From Dev to Prod: Containers-From Dev to Prod: Containers setup, Dev Environment Setup IDE, IDE-IDE standardization, Standardizing Dev Environments-Standardizing Dev Environments directed acyclic graph (DAG), Cron, Schedulers, and Orchestrators directional expectation tests, Directional expectation tests discretization, feature engineering and, Discretization-Discretization distributed training, Distributed Training data parallelism and, Data parallelism model parallelism and, Model parallelism-Model parallelism Docker Compose, From Dev to Prod: Containers Docker images, From Dev to Prod: Containers-From Dev to Prod: Containers Dockerfiles, From Dev to Prod: Containers-From Dev to Prod: Containers document model, Document model schemas, Document model downtime, Software System Failures driver management service, Data Passing Through Services dynamic sampling, Data-level methods: Resampling\n",
      " -What you believe to be the focus or the competitive advantages of your company Stefan Krawczyk, manager of the ML platform team at Stitch Fix, explained to me his build versus buy decision: “If it’s something we want to be really good at, we’ll manage that in-house. If not, we’ll use a vendor.”For the vast majority of companies outside the technology sector—e.g., companies in retail, banking, manufacturing—ML infrastructure isn’t their focus, so they\n",
      " -deploying a traditional software program. This difference might cause people who have never deployed a model before to either dread the process or underestimate how much time and effort it will take. In this section, we’ll debunk some of the common myths about the deployment process, which will, hopefully, put you in a good state of mind to begin the process. This section will be most helpful to people with little to no deploying experience. Myth 1: You Only Deploy One or Two ML Models at a Time When doing academic projects, I was advised to choose a small problem to focus on, which usually led to a single model. Many people from academic backgrounds I’ve talked to tend to also think of ML production in the context of a single model. Subsequently, the infrastructure they have in mind doesn’t work for actual applications, because it can only support one or two models. In reality, companies have many, many ML models. An application might have many different features, and each feature might require its own model. Consider a ride-sharing app like Uber.\n",
      " -\n"
     ]
    }
   ],
   "source": [
    "context=\"\"\n",
    "for indx in top_search_result_dot_product.indices.tolist():\n",
    "  context += cleaned_chunk[indx]['sentence_chunk'] + \"\\n -\"\n",
    "\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f96f50-3fd3-42bb-89a2-8184218438e7",
   "metadata": {
    "id": "86f96f50-3fd3-42bb-89a2-8184218438e7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3486fced-8d8a-49b8-bf59-bd24a5e5f108",
   "metadata": {
    "id": "3486fced-8d8a-49b8-bf59-bd24a5e5f108"
   },
   "source": [
    "### 5. Gerative LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "862f6c69-70ee-446f-890c-b9b737634898",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692,
     "referenced_widgets": [
      "8cb060c9ca4a450e9c8bd01f28dbba6d",
      "4fde9437f64b46b6bc3ff78965c4f382",
      "d1713ab1fdd8417a95e2b51aa6c04cc8",
      "269daecc72bd44f285129bd3bae852e7",
      "d15c701ea5bc4de9902e781b09703bee",
      "b5e040201a2e4f5899985b27d24dd890",
      "d0181985d35c41e8a7b8699f5a3606c4",
      "3929cdfa75fa4a108f240f4bb00a0726",
      "c6dba1072da0465899f42718dc01a4f4",
      "fe704bf4486143bba0101791a419cdc4",
      "6e75c5e610de41609d6a5aa28c3f8294",
      "8f9ca580fc2b483eab6db7f5d97b0bcf",
      "6f40ddd6da44453cba390824721318f3",
      "053529adcdc94c888b1ced1b6d9f5cb4",
      "7cf5524da9614e4cb8d3f170c60ae2fb",
      "7c20596c7c1b4a6aa990f0900e2ea4ca",
      "b33a09f22d3a455584e38f380130549c",
      "bbe762203c904f0192506e2863e590ee",
      "6048660342864ae2820658f8d2916ffb",
      "e6d336f6939f4a76818435c6cccd2c92",
      "825af0d5f7a743b89449d29d8b65d6b0",
      "dcd3aa9f01b14f589ea21256f1f26304",
      "f923aa37e7e34e3db2e836db562236c9",
      "3f28676e584f46119dc9f39b1738e492",
      "b49495dc485d49dd8625a711725ded6b",
      "f477207d5024416688b725f05c4cf118",
      "ae95019637234413991b81f41210bbae",
      "5491447134e64a2996089c370d405a39",
      "e38e39f7e9c84fec816375b96c1f7f15",
      "d2315f34662040d6a5e9808687759ee3",
      "60c4ef26faae4e868dabcc03f6690489",
      "49c5c251a52144c5aae86bb26acc96a9",
      "c87b7b1e29ab44b7aa78bfbfc27efc87",
      "4b067960d1774c9b9301ca8d2ad717ba",
      "a8f512aa8424439784ba637a0988129a",
      "0a2ef6071d9545b38a55d940ada4dc65",
      "eb596ec3697c4729855c6e73ddcd2ef5",
      "c25edb0d3e3e406f863ed24312c080fa",
      "898f4b3480054106a09664c38b832096",
      "e5c6dd4b20f041f882e2ebfa3b3f6c0b",
      "35695f8ed31c48fc84ed762335361dc8",
      "cda28e450fbb4b5896a60e2b30db0580",
      "66583e3eba0c4285a974fb3cd0579f31",
      "3f28f369e8f64d83aa0c5789759940e1",
      "19022c63f5f548149fa702d2e9aa5c32",
      "223e68e7f87a488f876bbfdea7636626",
      "6f99a10624ad4e699e5aab78e331c416",
      "794f962f9530447a9f9e34a17deccb88",
      "6129650143484f66865da88d4ade8255",
      "51200354808c424385ca1a2518d8ff31",
      "6dbb525a79b946f4aaeea219e955a279",
      "96b46e044e34433aa3e86e10a6449eb4",
      "9cbb2a6f766c41ba96d10e7832773291",
      "dab735a794db47b4adf913eb3f514590",
      "dcd136da9830456dba39bc2752de593e",
      "69a0521532e0457c857bfd7b3585aba0",
      "f2260152ed8c4cc98640cd399f701472",
      "dd03b692c43645febcecbd3ca1b98ffb",
      "1e0ab85f35c84afcbf1ca8b56789fcf4",
      "9f33b2e2d4f44fe69dafe513ef867152",
      "cddb3c9545244d3ba0f2d65a74385e74",
      "fcb1f05c075f424ca0553bb4e6d3ced2",
      "03ff02a71f634f108cd4b5d538cf8964",
      "812047f5865345aab25857cafba729c3",
      "622089cb3e1a406ca9088488c9beaf01",
      "0c705c0442934bb3803cd72383916aa9",
      "b40cb806c96d4bcda6e63f719695aa08",
      "a5244db9aed7400480c2c90d15bf2ae5",
      "c2c1e89e1fbc426f9380a160bc0a2587",
      "787ddd5985d244d59fe75ed3199f14f2",
      "39ae34e29f5c4c3e96b21bd3a4c9fb13",
      "376840a35bed441da72f0452f7323cbb",
      "16b247ec93614eef8fee24764c08b62e",
      "4698f3c49653403d9aaec147bf68f83d",
      "4f6f03ce85264306a1894f3c5a65a7f8",
      "172c736efcc24b45b8b0409f17638d20",
      "bdecf4bab1ee499385ed026a3f2fce37"
     ]
    },
    "id": "862f6c69-70ee-446f-890c-b9b737634898",
    "outputId": "ec89cbd2-a26a-4823-af6b-48a6a6036b13"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb060c9ca4a450e9c8bd01f28dbba6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9ca580fc2b483eab6db7f5d97b0bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f923aa37e7e34e3db2e836db562236c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b067960d1774c9b9301ca8d2ad717ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19022c63f5f548149fa702d2e9aa5c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a0521532e0457c857bfd7b3585aba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40cb806c96d4bcda6e63f719695aa08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ea0e2-4489-4a0e-a541-6b6063d26345",
   "metadata": {
    "id": "586ea0e2-4489-4a0e-a541-6b6063d26345"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a519da-b7e2-4537-a276-1b96d251d160",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2a519da-b7e2-4537-a276-1b96d251d160",
    "outputId": "215dddba-fa65-4856-cf76-e1097630da94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "531cb968-ead8-41f3-9ffa-1a8994099a9b",
   "metadata": {
    "id": "531cb968-ead8-41f3-9ffa-1a8994099a9b"
   },
   "outputs": [],
   "source": [
    "def generate_answer(context, question=query, max_length=512):\n",
    "    prompt = (\n",
    "    f\"As an expert, answer the following question use the Context provided.\\n\"\n",
    "    # f\"Be accurate, concise, give examples and steps .\\n\\n\"\n",
    "    f\"Context:\\n{context}\\n\\n\"\n",
    "    f\"Question:\\n{question}\\n\\n\"\n",
    "    f\"Answer:\"\n",
    ")\n",
    "\n",
    "    # prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, max_new_tokens=max_length, do_sample=True, temperature=0.7)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d38a0dd-959e-4205-9bdf-506b6cfa631a",
   "metadata": {
    "id": "7d38a0dd-959e-4205-9bdf-506b6cfa631a"
   },
   "outputs": [],
   "source": [
    "# context = context.to(device)\n",
    "# query = query.to(device)\n",
    "answer = generate_answer(context, query, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fd96d49-b8c8-4ad5-8fc9-c8e21a5e819c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fd96d49-b8c8-4ad5-8fc9-c8e21a5e819c",
    "outputId": "60dab98e-ae36-4848-dfd5-8af43ab0a743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an expert, answer the following question use the Context provided.\n",
      "Context:\n",
      "Chapter 10. Infrastructure and Tooling for MLOps In Chapters 4 to 6, we discussed the logic for developing ML systems. In Chapters 7 to 9, we discussed the considerations for deploying, monitoring, and continually updating an ML system. Up until now, we’ve assumed that ML practitioners have access to all the tools and infrastructure they need to implement that logic and carry out these considerations. However, that assumption is far from being true. Many data scientists have told me that they know the right things to do for their ML systems, but they can’t do them because their infrastructure isn’t set up in a way that enables them to do so. ML systems are complex. The more complex a system, the more it can benefit from good infrastructure. Infrastructure, when set up right, can help automate processes, reducing the need for specialized knowledge and engineering time. This, in turn, can speed up the development and delivery of ML applications, reduce the surface area for bugs, and enable new use cases.\n",
      " -When set up wrong, however, infrastructure is painful to use and expensive to replace. In this chapter, we’ll discuss how to set up infrastructure right for ML systems. Before we dive in, it’s important to note that every company’s infrastructure needs are different. The infrastructure required for you depends on the number of applications you develop and how specialized the applications are. At one end of the spectrum, you have companies that use ML for ad hoc business analytics such as to project the number of new users they’ll have next year to present at their quarterly planning meeting. These companies probably won’t need to invest in any infrastructure—Jupyter Notebooks, Python, and Pandas would be their best friends. If you have only one simple ML use case, such as an Android app for object detection to show your friends, you probably won’t need any infrastructure either—you just need an Android-compatible ML framework like TensorFlow Lite. At the other end of the spectrum, there are companies that work on applications with unique requirements. For example, self-driving cars have unique accuracy\n",
      " -scale, Myth 4: Most ML Engineers Don’t Need to Worry About Scale separation of responsibilities, Model Deployment and Prediction Service shadow deployment, Shadow Deployment development environment, infrastructure, Infrastructure and Tooling for MLOps, Development Environment containers, From Dev to Prod: Containers-From Dev to Prod: Containers setup, Dev Environment Setup IDE, IDE-IDE standardization, Standardizing Dev Environments-Standardizing Dev Environments directed acyclic graph (DAG), Cron, Schedulers, and Orchestrators directional expectation tests, Directional expectation tests discretization, feature engineering and, Discretization-Discretization distributed training, Distributed Training data parallelism and, Data parallelism model parallelism and, Model parallelism-Model parallelism Docker Compose, From Dev to Prod: Containers Docker images, From Dev to Prod: Containers-From Dev to Prod: Containers Dockerfiles, From Dev to Prod: Containers-From Dev to Prod: Containers document model, Document model schemas, Document model downtime, Software System Failures driver management service, Data Passing Through Services dynamic sampling, Data-level methods: Resampling\n",
      " -What you believe to be the focus or the competitive advantages of your company Stefan Krawczyk, manager of the ML platform team at Stitch Fix, explained to me his build versus buy decision: “If it’s something we want to be really good at, we’ll manage that in-house. If not, we’ll use a vendor.”For the vast majority of companies outside the technology sector—e.g., companies in retail, banking, manufacturing—ML infrastructure isn’t their focus, so they\n",
      " -deploying a traditional software program. This difference might cause people who have never deployed a model before to either dread the process or underestimate how much time and effort it will take. In this section, we’ll debunk some of the common myths about the deployment process, which will, hopefully, put you in a good state of mind to begin the process. This section will be most helpful to people with little to no deploying experience. Myth 1: You Only Deploy One or Two ML Models at a Time When doing academic projects, I was advised to choose a small problem to focus on, which usually led to a single model. Many people from academic backgrounds I’ve talked to tend to also think of ML production in the context of a single model. Subsequently, the infrastructure they have in mind doesn’t work for actual applications, because it can only support one or two models. In reality, companies have many, many ML models. An application might have many different features, and each feature might require its own model. Consider a ride-sharing app like Uber.\n",
      " -\n",
      "\n",
      "Question:\n",
      "How can infrastructure help reduce engineering effort in ML deployment?\n",
      "\n",
      "Answer:\n",
      "Infrastructure helps reduce engineering effort in ML deployment by enabling teams to automate process, reducing the need for specialized knowledge and engineering time. The more complex a system, the more it can benefit from good infrastructure. Infrastructure can help automate processes, reducing the need for specialized knowledge and engineering time. This, in turn, can speed up the development and delivery of ML applications, reduce the surface area for bugs, and enable new use cases.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd7bfc-f686-46fc-bf17-ce26b6ba77c6",
   "metadata": {
    "id": "3bbd7bfc-f686-46fc-bf17-ce26b6ba77c6"
   },
   "outputs": [],
   "source": [
    "#  # from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "# from transformers import pipeline\n",
    "\n",
    "# # model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# # model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# question = query\n",
    "# def generate_answer(question, context):\n",
    "#     prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "#     response = generator(prompt, max_new_tokens=512, temperature=0.7)\n",
    "#     return response[0][\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a8d7f8-973c-4e64-89b6-c165aae549fe",
   "metadata": {
    "id": "17a8d7f8-973c-4e64-89b6-c165aae549fe"
   },
   "outputs": [],
   "source": [
    "# answer = generate_answer(context, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77ae4d-9709-40ad-8c1e-2ca54b81bc7d",
   "metadata": {
    "id": "6a77ae4d-9709-40ad-8c1e-2ca54b81bc7d"
   },
   "outputs": [],
   "source": [
    "# answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b83ab-9d78-4cfc-8f43-37f81e611d45",
   "metadata": {
    "id": "6d4b83ab-9d78-4cfc-8f43-37f81e611d45"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
